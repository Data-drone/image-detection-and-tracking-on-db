{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Video Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U opencv-python timm transformers\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to unpack a single video then to run the DET algorithm and return the bounding boxes.\n",
    "At this moment this runs serially\n",
    "\n",
    "We can investigate ways in order to run this in parallel\n",
    "We can also leverage a LLM in order to annotate and describe the image if we so wish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup configs\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "catalog = 'brian_ml_dev'\n",
    "schema = 'image_processing'\n",
    "raw_data = 'raw_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_folder = f'/Volumes/{catalog}/{schema}/{raw_data}'\n",
    "files = os.listdir(video_folder)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test read first file\n",
    "first_file = os.path.join( video_folder, files[0] )\n",
    "capture = cv2.VideoCapture(first_file)\n",
    "\n",
    "frames = []\n",
    "frame_index = 0\n",
    "\n",
    "while True:\n",
    "    success, frame = capture.read()\n",
    "    if not success:\n",
    "        break\n",
    "    frames.append(frame)\n",
    "    frame_index += 1\n",
    "\n",
    "capture.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "bgr_image_array = frames[0]\n",
    "rgb_array = cv2.cvtColor(bgr_image_array, cv2.COLOR_BGR2RGB)\n",
    "pil_image = Image.fromarray(rgb_array)\n",
    "display(pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrFeatureExtractor, DetrForObjectDetection\n",
    "\n",
    "# Load model and feature extractor\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = feature_extractor(images=pil_image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "results = feature_extractor.post_process_object_detection(outputs, \n",
    "                                                          target_sizes=torch.tensor([(pil_image.height, pil_image.width)]), threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for result in results:\n",
    "    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "        score, label = score.item(), label_id.item()\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rgb_image = np.array(bgr_image_array)\n",
    "\n",
    "for result in results:\n",
    "    for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n",
    "        x1, y1, x2, y2 = map(int, box.tolist())\n",
    "        cv2.rectangle(rgb_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        text = f\"{model.config.id2label[label_id.item()]} {score:.2f}\"\n",
    "        cv2.putText(rgb_image, text, (x1, max(0, y1 - 5)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv2_imshow(a):\n",
    "  \"\"\"A replacement for cv2.imshow() for use in Jupyter notebooks.\n",
    "\n",
    "  Args:\n",
    "    a : np.ndarray. shape (N, M) or (N, M, 1) is an NxM grayscale image. shape\n",
    "      (N, M, 3) is an NxM BGR color image. shape (N, M, 4) is an NxM BGRA color\n",
    "      image.\n",
    "  \"\"\"\n",
    "  a = a.clip(0, 255).astype('uint8')\n",
    "  # cv2 stores colors as BGR; convert to RGB\n",
    "  if a.ndim == 3:\n",
    "    if a.shape[2] == 4:\n",
    "      a = cv2.cvtColor(a, cv2.COLOR_BGRA2RGBA)\n",
    "    else:\n",
    "      a = cv2.cvtColor(a, cv2.COLOR_BGR2RGB)\n",
    "  display(Image.fromarray(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2_imshow(rgb_image)\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
