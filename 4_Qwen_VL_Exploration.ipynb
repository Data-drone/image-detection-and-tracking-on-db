{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Video Exploration with Qwen3-VL-4B-Instruct\n",
        "\n",
        "This notebook demonstrates how to use the **Qwen3-VL-4B-Instruct** vision-language model to analyze and describe video frames. \n",
        "\n",
        "Qwen3-VL is a state-of-the-art multimodal model from Alibaba's Qwen team that can:\n",
        "- Understand images and videos\n",
        "- Answer questions about visual content\n",
        "- Perform OCR in 32 languages\n",
        "- Handle spatial grounding and visual reasoning\n",
        "- Support up to 256K token context (expandable to 1M)\n",
        "\n",
        "Reference: [Qwen3-VL GitHub](https://github.com/QwenLM/Qwen3-VL) | [Model Card](https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages for Databricks MLR 16.4 LTS\n",
        "%pip install -U opencv-python-headless transformers accelerate qwen-vl-utils numpy<2\n",
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, let's configure the location of video files and UC Catalog/Schema.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "catalog = 'brian_ml_dev'\n",
        "schema = 'image_processing'\n",
        "raw_data = 'raw_data'\n",
        "\n",
        "video_folder = f'/Volumes/{catalog}/{schema}/{raw_data}'\n",
        "print(f\"Video folder: {video_folder}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## List Available Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List all video files\n",
        "files = os.listdir(video_folder)\n",
        "print(f\"Found {len(files)} video file(s):\")\n",
        "for idx, file in enumerate(files):\n",
        "    print(f\"  {idx}: {file}\")\n",
        "\n",
        "files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Video and Extract Frames\n",
        "\n",
        "Let's load the first video and extract some sample frames for analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select first video file\n",
        "video_file = os.path.join(video_folder, files[0])\n",
        "print(f\"Selected video: {files[0]}\")\n",
        "\n",
        "# Open video capture\n",
        "capture = cv2.VideoCapture(video_file)\n",
        "\n",
        "# Get video properties\n",
        "fps = capture.get(cv2.CAP_PROP_FPS)\n",
        "total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "duration = total_frames / fps if fps > 0 else 0\n",
        "\n",
        "print(f\"Video properties:\")\n",
        "print(f\"  FPS: {fps}\")\n",
        "print(f\"  Total frames: {total_frames}\")\n",
        "print(f\"  Duration: {duration:.2f} seconds\")\n",
        "\n",
        "# Extract frames at intervals (e.g., every 30 frames or 1 second for 30fps video)\n",
        "frame_interval = int(fps) if fps > 0 else 30\n",
        "sample_frames = []\n",
        "frame_indices = []\n",
        "\n",
        "frame_index = 0\n",
        "while True:\n",
        "    success, frame = capture.read()\n",
        "    if not success:\n",
        "        break\n",
        "    \n",
        "    # Sample frames at intervals (max 10 frames for this demo)\n",
        "    if frame_index % frame_interval == 0 and len(sample_frames) < 10:\n",
        "        sample_frames.append(frame)\n",
        "        frame_indices.append(frame_index)\n",
        "    \n",
        "    frame_index += 1\n",
        "\n",
        "capture.release()\n",
        "\n",
        "print(f\"\\nExtracted {len(sample_frames)} sample frames at indices: {frame_indices}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Sample Frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first sample frame\n",
        "if len(sample_frames) > 0:\n",
        "    bgr_image = sample_frames[0]\n",
        "    rgb_array = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(rgb_array)\n",
        "    display(pil_image)\n",
        "    print(f\"Frame {frame_indices[0]} - Image size: {pil_image.size}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Qwen3-VL-4B-Instruct Model\n",
        "\n",
        "Qwen3-VL-4B-Instruct is a compact 4.8B parameter vision-language model that:\n",
        "- Uses the same architecture as Qwen2-VL (hence the `Qwen2VLForConditionalGeneration` class)\n",
        "- Supports multimodal inputs (text, images, and videos)\n",
        "- Uses `qwen-vl-utils` for processing vision inputs\n",
        "- Optimized for efficient inference with bfloat16 precision\n",
        "\n",
        "The model uses a chat template format with structured messages containing both text and vision inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# Load the Qwen3-VL-4B-Instruct model\n",
        "# Note: Qwen3-VL uses the same Qwen2VLForConditionalGeneration class architecture\n",
        "model_name = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
        "\n",
        "print(\"Loading Qwen3-VL-4B-Instruct model...\")\n",
        "print(\"This may take a few minutes on first run...\")\n",
        "\n",
        "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model device: {model.device}\")\n",
        "print(f\"Model dtype: {model.dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Frame with Qwen VL\n",
        "\n",
        "Let's ask the model to describe what's happening in the video frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_frame_with_qwen(pil_image, prompt=\"Describe this image in detail.\"):\n",
        "    \"\"\"\n",
        "    Analyze a single frame using Qwen VL model.\n",
        "    \n",
        "    Args:\n",
        "        pil_image: PIL Image object\n",
        "        prompt: Question or instruction for the model\n",
        "    \n",
        "    Returns:\n",
        "        Generated text description\n",
        "    \"\"\"\n",
        "    # Prepare the conversation format\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"image\",\n",
        "                    \"image\": pil_image,\n",
        "                },\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Prepare for inference\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    \n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(model.device)\n",
        "    \n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
        "    \n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    \n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    \n",
        "    return output_text[0]\n",
        "\n",
        "print(\"Analysis function ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze first frame\n",
        "if len(sample_frames) > 0:\n",
        "    print(\"Analyzing first frame...\\n\")\n",
        "    \n",
        "    # Convert frame to PIL Image\n",
        "    bgr_image = sample_frames[0]\n",
        "    rgb_array = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(rgb_array)\n",
        "    \n",
        "    # Get detailed description\n",
        "    description = analyze_frame_with_qwen(pil_image, \"Describe this image in detail. What objects, people, or activities do you see?\")\n",
        "    \n",
        "    print(f\"Frame {frame_indices[0]} Analysis:\")\n",
        "    print(\"=\" * 80)\n",
        "    print(description)\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze Multiple Frames\n",
        "\n",
        "Let's analyze several frames from the video to understand the temporal progression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze multiple frames with different prompts\n",
        "analysis_results = []\n",
        "\n",
        "prompts = [\n",
        "    \"What is the main subject of this image?\",\n",
        "    \"Describe the scene and any notable activities.\",\n",
        "    \"What objects can you identify in this image?\",\n",
        "    \"Describe the environment and setting.\",\n",
        "]\n",
        "\n",
        "num_frames_to_analyze = min(4, len(sample_frames))\n",
        "\n",
        "for i in range(num_frames_to_analyze):\n",
        "    bgr_image = sample_frames[i]\n",
        "    rgb_array = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(rgb_array)\n",
        "    \n",
        "    prompt = prompts[i % len(prompts)]\n",
        "    \n",
        "    print(f\"\\nAnalyzing Frame {frame_indices[i]}...\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    \n",
        "    result = analyze_frame_with_qwen(pil_image, prompt)\n",
        "    \n",
        "    analysis_results.append({\n",
        "        'frame_index': frame_indices[i],\n",
        "        'prompt': prompt,\n",
        "        'response': result\n",
        "    })\n",
        "    \n",
        "    print(f\"Response: {result}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "print(f\"\\nCompleted analysis of {len(analysis_results)} frames\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Query Interface\n",
        "\n",
        "Try asking your own questions about the frames!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive analysis - change this prompt to ask different questions\n",
        "custom_prompt = \"Count the number of people visible in this image.\"\n",
        "\n",
        "if len(sample_frames) > 0:\n",
        "    bgr_image = sample_frames[0]\n",
        "    rgb_array = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
        "    pil_image = Image.fromarray(rgb_array)\n",
        "    \n",
        "    print(f\"Query: {custom_prompt}\\n\")\n",
        "    response = analyze_frame_with_qwen(pil_image, custom_prompt)\n",
        "    print(f\"Response: {response}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Direct Video Analysis (No Frame Decomposition)\n",
        "\n",
        "Qwen3-VL natively supports video inputs! Instead of manually extracting frames, we can pass the video file directly to the model. This approach:\n",
        "- Leverages Qwen3-VL's temporal understanding across the entire video\n",
        "- Uses the 256K token context window for long-form video comprehension\n",
        "- Eliminates the need for manual frame sampling\n",
        "- Provides holistic video understanding rather than frame-by-frame analysis\n",
        "\n",
        "Based on the [qwen-vl-utils examples](https://github.com/QwenLM/Qwen3-VL/tree/main/qwen-vl-utils), we can use video paths directly in the message format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_video_direct(video_path, prompt=\"Describe what happens in this video in detail.\"):\n",
        "    \"\"\"\n",
        "    Analyze an entire video using Qwen3-VL's native video understanding.\n",
        "    No frame extraction needed - the model processes the video directly.\n",
        "    \n",
        "    Args:\n",
        "        video_path: Path to the video file\n",
        "        prompt: Question or instruction for the model\n",
        "    \n",
        "    Returns:\n",
        "        Generated text description of the video\n",
        "    \"\"\"\n",
        "    # Prepare the conversation format with video input\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\n",
        "                    \"type\": \"video\",\n",
        "                    \"video\": video_path,  # Direct video path\n",
        "                    \"max_pixels\": 360 * 420,  # Control video resolution for memory efficiency\n",
        "                    \"fps\": 1.0,  # Sample rate for video frames (1 fps = 1 frame per second)\n",
        "                },\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    # Prepare for inference\n",
        "    text = processor.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    image_inputs, video_inputs = process_vision_info(messages)\n",
        "    \n",
        "    inputs = processor(\n",
        "        text=[text],\n",
        "        images=image_inputs,\n",
        "        videos=video_inputs,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    inputs = inputs.to(model.device)\n",
        "    \n",
        "    # Generate response\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(**inputs, max_new_tokens=1024)\n",
        "    \n",
        "    generated_ids_trimmed = [\n",
        "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    \n",
        "    output_text = processor.batch_decode(\n",
        "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "    )\n",
        "    \n",
        "    return output_text[0]\n",
        "\n",
        "print(\"Direct video analysis function ready!\")\n",
        "print(\"Note: Processing full videos requires more memory than single frames.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze the first video directly (without frame extraction)\n",
        "if len(files) > 0:\n",
        "    print(f\"Analyzing video: {files[0]}\")\n",
        "    print(\"This may take a moment as the model processes the entire video...\\n\")\n",
        "    \n",
        "    # Analyze with different prompts to showcase temporal understanding\n",
        "    video_prompts = [\n",
        "        \"Describe what happens in this video in detail.\",\n",
        "        \"What is the main activity or scene shown in this video?\",\n",
        "        \"Summarize the key events and objects visible throughout this video.\",\n",
        "    ]\n",
        "    \n",
        "    for i, prompt in enumerate(video_prompts):\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Query {i+1}: {prompt}\")\n",
        "        print('='*80)\n",
        "        \n",
        "        result = analyze_video_direct(video_file, prompt)\n",
        "        \n",
        "        print(f\"\\nResponse:\\n{result}\")\n",
        "        \n",
        "        if i < len(video_prompts) - 1:\n",
        "            print(\"\\n\" + \"-\"*80)\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(\"Direct video analysis complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison: Frame-by-Frame vs Direct Video Analysis\n",
        "\n",
        "**Frame-by-Frame Analysis (OpenCV):**\n",
        "- ✅ Fine-grained control over which frames to analyze\n",
        "- ✅ Lower memory usage per inference call\n",
        "- ✅ Can process specific moments in time\n",
        "- ❌ Loses temporal context between frames\n",
        "- ❌ Requires manual frame extraction and management\n",
        "- ❌ Multiple API calls for multiple frames\n",
        "\n",
        "**Direct Video Analysis (Native Qwen3-VL):**\n",
        "- ✅ Holistic understanding with temporal context\n",
        "- ✅ Automatic frame sampling by the model\n",
        "- ✅ Single API call for entire video\n",
        "- ✅ Leverages full 256K context window\n",
        "- ❌ Higher memory requirements\n",
        "- ❌ Less control over specific frames\n",
        "- ❌ May be slower for very long videos\n",
        "\n",
        "**Recommendation:** Use direct video analysis for understanding overall video content and temporal relationships. Use frame-by-frame for detailed analysis of specific moments or when memory is constrained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results to DataFrame\n",
        "\n",
        "Let's structure our analysis results and save them for further processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create pandas DataFrame from results\n",
        "results_df = pd.DataFrame(analysis_results)\n",
        "results_df['video_file'] = files[0]\n",
        "\n",
        "display(results_df)\n",
        "\n",
        "print(f\"\\nDataFrame shape: {results_df.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save to Delta Table (Optional)\n",
        "\n",
        "We can save our analysis results to a Delta table for future reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to Spark DataFrame\n",
        "spark_df = spark.createDataFrame(results_df)\n",
        "\n",
        "# Define table name\n",
        "results_table = f\"{catalog}.{schema}.qwen_vl_video_analysis\"\n",
        "\n",
        "# Save to Delta table\n",
        "spark_df.write.mode('append').saveAsTable(results_table)\n",
        "\n",
        "print(f\"Results saved to: {results_table}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. Loading video files from Unity Catalog Volumes\n",
        "2. **Frame-by-Frame Analysis:** Extracting sample frames from videos using OpenCV for detailed moment analysis\n",
        "3. **Direct Video Analysis:** Using Qwen3-VL's native video input for temporal understanding (no frame extraction needed)\n",
        "4. Using **Qwen3-VL-4B-Instruct** for visual understanding with custom prompts\n",
        "5. Processing vision inputs with `qwen-vl-utils` (supports both image and video types)\n",
        "6. Comparing two approaches: frame-level vs video-level analysis\n",
        "7. Structuring and saving results to Delta tables\n",
        "\n",
        "### Key Insights\n",
        "- Qwen3-VL-4B is a compact yet powerful multimodal model (4.8B params)\n",
        "- Supports 256K context window (expandable to 1M tokens)\n",
        "- **Natively handles both images and videos** - can process entire videos with temporal context\n",
        "- Uses structured message format for multimodal inputs\n",
        "- Frame-by-frame approach offers precision; direct video analysis offers holistic understanding\n",
        "\n",
        "### Next Steps\n",
        "- Batch process multiple videos using Spark for distributed inference\n",
        "- Implement hybrid approach: direct video for summary + frame analysis for key moments\n",
        "- Fine-tune the model on domain-specific data (traffic, surveillance, etc.)\n",
        "- Combine with object detection (DETR) for comprehensive analysis\n",
        "- Optimize video processing parameters (fps, max_pixels) for different use cases\n",
        "- Explore multi-video comparison and temporal event detection\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
