{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finetuning with Ultralytics YOLO Multi-GPU\n",
        "\n",
        "This notebook demonstrates fine-tuning the latest Ultralytics YOLO models (YOLO11/YOLOv8) on custom datasets in a distributed manner.\n",
        "\n",
        "**References:**\n",
        "- https://docs.ultralytics.com/modes/train/\n",
        "- https://docs.ultralytics.com/datasets/detect/\n",
        "\n",
        "**Prerequisites**\n",
        "- MLR 17.3 LTS - need numpy 2.z compatibility\n",
        "- Cluster with Multi-GPU\n",
        "- Cluster started with `scripts/init_script_ultralytics.sh` init script\n",
        "- **YOLO dataset already prepared** with:\n",
        "  - `data.yaml` config file in your dataset volume\n",
        "  - Images in `images/` directory\n",
        "  - Labels in `labels/` directory (YOLO format: class_id center_x center_y width height) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U mlflow psutil nvidia-ml-py\n",
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup and Configure\n",
        "\n",
        "Initialize all variables needed for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Dataset configuration\n",
        "ds_catalog = 'brian_ml_dev'\n",
        "ds_schema = 'image_processing'\n",
        "dataset_volume = 'coco_dataset'\n",
        "logging_folder = 'training'\n",
        "\n",
        "# Paths\n",
        "dataset_path = f\"/Volumes/{ds_catalog}/{ds_schema}/{dataset_volume}\"\n",
        "config_path = f\"{dataset_path}/data.yaml\"  # YOLO config file (must exist)\n",
        "training_volume_path = f\"/local_disk0/ultralytics_logging_folder\"\n",
        "logging_vol_path = f\"/Volumes/{ds_catalog}/{ds_schema}/{logging_folder}\"\n",
        "\n",
        "# MLflow\n",
        "mlflow_experiment = '/Users/brian.law@databricks.com/brian_yolo_training'\n",
        "\n",
        "# MLflow connectivity\n",
        "browser_host = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
        "db_host = f\"https://{browser_host}\"\n",
        "db_token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
        "workspace_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply(\"orgId\")\n",
        "\n",
        "# YOLO model to start from (yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt)\n",
        "YOLO_MODEL = 'yolo11n.pt'\n",
        "\n",
        "print(f\"Dataset location: {dataset_path}\")\n",
        "print(f\"Config file: {config_path}\")\n",
        "print(f\"Training outputs: {training_volume_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Databricks widgets for hyperparameters\n",
        "dbutils.widgets.text(\"epochs\", \"2\", \"Epochs\")\n",
        "dbutils.widgets.text(\"batch_size\", \"128\", \"Batch Size\")\n",
        "dbutils.widgets.text(\"img_size\", \"640\", \"Image Size\")\n",
        "dbutils.widgets.text(\"initial_lr\", \"0.005\", \"Initial Learning Rate\")\n",
        "dbutils.widgets.text(\"final_lr\", \"0.1\", \"Final LR Factor\")\n",
        "dbutils.widgets.text(\"device_config\", \"[0,1]\", \"Device Config (e.g., [0,1])\")\n",
        "dbutils.widgets.text(\"run_name\", \"multi_gpu_run\", \"Run Name\")\n",
        "\n",
        "# Get hyperparameters from widgets\n",
        "EPOCHS = int(dbutils.widgets.get(\"epochs\"))\n",
        "BATCH_SIZE = int(dbutils.widgets.get(\"batch_size\"))\n",
        "IMG_SIZE = int(dbutils.widgets.get(\"img_size\"))\n",
        "initial_lr = float(dbutils.widgets.get(\"initial_lr\"))\n",
        "final_lr = float(dbutils.widgets.get(\"final_lr\"))\n",
        "device_config = eval(dbutils.widgets.get(\"device_config\"))  # Parse list from string\n",
        "run_name = dbutils.widgets.get(\"run_name\")\n",
        "\n",
        "print(\"Training Hyperparameters:\")\n",
        "print(f\"  Epochs: {EPOCHS}\")\n",
        "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"  Image Size: {IMG_SIZE}\")\n",
        "print(f\"  Initial LR: {initial_lr}\")\n",
        "print(f\"  Final LR Factor: {final_lr}\")\n",
        "print(f\"  Device Config: {device_config}\")\n",
        "print(f\"  Run Name: {run_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Dataset\n",
        "\n",
        "Verify that the YOLO dataset is properly configured and ready for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "# Verify config file exists\n",
        "if not Path(config_path).exists():\n",
        "    raise FileNotFoundError(f\"Config file not found: {config_path}\\n\"\n",
        "                          f\"Please run dataset preparation first.\")\n",
        "\n",
        "# Load and display config\n",
        "with open(config_path, 'r') as f:\n",
        "    yolo_config = yaml.safe_load(f)\n",
        "\n",
        "print(\"YOLO Dataset Configuration:\")\n",
        "print(f\"  Path: {yolo_config['path']}\")\n",
        "print(f\"  Train: {yolo_config['train']}\")\n",
        "print(f\"  Val: {yolo_config['val']}\")\n",
        "print(f\"  Classes: {yolo_config['nc']}\")\n",
        "print(f\"  Class names (first 10): {yolo_config['names'][:10]}...\")\n",
        "print(f\"\\n✓ Dataset configuration validated!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multi-GPU Training with TorchDistributor\n",
        "\n",
        "For multi-GPU training on Databricks, we use `TorchDistributor` which properly manages distributed processes. This is required because Ultralytics' built-in `device=[0,1]` approach doesn't work in Databricks notebook environments.\n",
        "\n",
        "**How it works:**\n",
        "- TorchDistributor spawns one process per GPU\n",
        "- Each process runs the training function with its own `local_rank`\n",
        "- NCCL handles communication between GPUs\n",
        "- Only rank 0 handles MLflow logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_count = torch.cuda.device_count()\n",
        "    print(f\"Available GPUs: {gpu_count}\")\n",
        "    for i in range(gpu_count):\n",
        "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "else:\n",
        "    raise RuntimeError(\"CUDA not available. This notebook requires GPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare configuration for training script\n",
        "import json\n",
        "\n",
        "# Extract Databricks job run ID (works in job and interactive contexts)\n",
        "try:\n",
        "    #job_run_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"runId\").get()\n",
        "    job_run_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply(\"multitaskParentRunId\")\n",
        "    run_id = job_run_id\n",
        "except:\n",
        "    job_run_id = \"no_id\"\n",
        "    run_id = None\n",
        "\n",
        "try:\n",
        "    #job_run_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"runId\").get()\n",
        "    job_id = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply(\"jobId\")\n",
        "except:\n",
        "    job_id = None\n",
        "\n",
        "\n",
        "print(f\"Job Run ID: {job_run_id}\")\n",
        "\n",
        "config = {\n",
        "    'model': YOLO_MODEL,\n",
        "    'data_config': config_path,\n",
        "    'epochs': EPOCHS,\n",
        "    'batch_size': BATCH_SIZE,\n",
        "    'img_size': IMG_SIZE,\n",
        "    'initial_lr': initial_lr,\n",
        "    'final_lr': final_lr,\n",
        "    'run_name': run_name,\n",
        "    'project_path': training_volume_path,\n",
        "    'mlflow_experiment': mlflow_experiment,\n",
        "    'db_host': db_host,\n",
        "    'db_token': db_token,\n",
        "    'db_workspace_id': workspace_id,\n",
        "    'databricks_job_id': job_id,\n",
        "    'databricks_run_id': run_id,\n",
        "    'dataset_path': dataset_path,\n",
        "    'num_classes': yolo_config['nc'],\n",
        "    'dataset_name': f\"{ds_catalog}.{ds_schema}.{dataset_volume}\",\n",
        "    'patience': 50,\n",
        "    'save_period': 5\n",
        "}\n",
        "\n",
        "# Script path for TorchDistributor\n",
        "train_script_path = \"../scripts/train_yolo.py\"\n",
        "config_json = json.dumps(config)\n",
        "\n",
        "print(\"\\nTraining Configuration:\")\n",
        "for key, value in config.items():\n",
        "    if 'token' not in key.lower():  # Don't print tokens\n",
        "        print(f\"  {key}: {value}\")\n",
        "print(f\"\\nTraining script: {train_script_path}\")\n",
        "print(f\"Metrics output directory: {logging_vol_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.torch.distributor import TorchDistributor\n",
        "import time\n",
        "\n",
        "# Set MLflow experiment\n",
        "mlflow.set_experiment(mlflow_experiment)\n",
        "\n",
        "# Create TorchDistributor and run training\n",
        "num_processes = len(device_config) if isinstance(device_config, list) else 1\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Starting distributed training with {num_processes} GPUs\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "# Start MLflow run with system metrics logging\n",
        "with mlflow.start_run(run_name=run_name, log_system_metrics=True) as run:\n",
        "    active_run_id = run.info.run_id\n",
        "    print(f\"MLflow Run ID: {active_run_id}\\n\")\n",
        "    \n",
        "    # Log hyperparameters upfront\n",
        "    mlflow.log_params({\n",
        "        'model': YOLO_MODEL,\n",
        "        'epochs': EPOCHS,\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'img_size': IMG_SIZE,\n",
        "        'initial_lr': initial_lr,\n",
        "        'final_lr': final_lr,\n",
        "        'num_gpus': num_processes,\n",
        "        'device_config': str(device_config),\n",
        "        'run_name': run_name,\n",
        "        'dataset_path': dataset_path,\n",
        "    })\n",
        "    \n",
        "    # Create distributor\n",
        "    distributor = TorchDistributor(\n",
        "        num_processes=num_processes,\n",
        "        local_mode=True,  # Single node multi-GPU\n",
        "        use_gpu=True\n",
        "    )\n",
        "    \n",
        "    # Run distributed training using script\n",
        "    print(\"Starting TorchDistributor execution...\")\n",
        "    print(f\"  - MLflow Run ID: {active_run_id}\")\n",
        "    print(f\"  - Job Run ID: {job_run_id}\")\n",
        "    print(f\"  - Metrics output: {logging_vol_path}/metrics_{job_run_id}.json\\n\")\n",
        "    \n",
        "    # Pass all required parameters to training script\n",
        "    output = distributor.run(train_script_path, active_run_id, config_json, job_run_id, logging_vol_path)\n",
        "\n",
        "# Display training results\n",
        "if output:\n",
        "    print(\"\\nTraining Results:\")\n",
        "    for key, value in output.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(\"\\nTraining completed (no metrics returned from worker processes)\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Training complete! All MLflow logging finished.\")\n",
        "print(f\"View results at: {mlflow_experiment}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Small delay to ensure all background processes fully exit\n",
        "time.sleep(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Complete!\n",
        "\n",
        "The training function above includes all MLflow integration:\n",
        "\n",
        "**What's Logged:**\n",
        "- ✅ **Hyperparameters** - Model config, training settings, dataset info\n",
        "- ✅ **Dataset Lineage** - Dataset reference with class information  \n",
        "- ✅ **Training Metrics** - Ultralytics built-in MLflow callback logs metrics automatically\n",
        "- ✅ **Wrapped Model** - Deployment-ready PyTorch model with proper signature\n",
        "- ✅ **System Metrics** - GPU/CPU/memory usage during training\n",
        "\n",
        "**Next Steps:**\n",
        "- View results in MLflow UI at the experiment path above\n",
        "- Uncomment `registered_model_name` in the training function to register the model to Unity Catalog\n",
        "- Use the logged model artifact for deployment or further evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
