{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finetuning with Ultralytics YOLO\n",
        "\n",
        "This notebook demonstrates fine-tuning the latest Ultralytics YOLO models (YOLO11/YOLOv8) on custom datasets using single GPU training.\n",
        "\n",
        "**Key Features:**\n",
        "- Simple API with minimal boilerplate\n",
        "- Fast training and inference\n",
        "- Native COCO format support\n",
        "- Production-ready MLflow integration\n",
        "- Deployment-ready model wrapper with base64 input support\n",
        "\n",
        "**References:**\n",
        "- https://docs.ultralytics.com/modes/train/\n",
        "- https://docs.ultralytics.com/datasets/detect/\n",
        "\n",
        "**Prerequisites**\n",
        "- MLR 17.3 LTS (for numpy 2.x compatibility)\n",
        "- Single GPU cluster\n",
        "- Cluster started with `scripts/init_script_ultralytics.sh` init script\n",
        "- COCO format dataset created from video processing notebooks:\n",
        "  - `1_Processing_w_ffmpeg.ipynb` - standardize input videos\n",
        "  - `2_Batch_Inference_w_opencv.ipynb` - create COCO annotations "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sh\n",
        "# Make sure to install these via init scripts for sustainability\n",
        "# /databricks/python/bin/pip install -U ultralytics supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U mlflow psutil nvidia-ml-py\n",
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup and Configure\n",
        "\n",
        "Initialize all variables needed for training using Unity Catalog Volumes for data storage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "ds_catalog = 'brian_ml_dev'\n",
        "ds_schema = 'image_processing'\n",
        "coco_volume = 'coco_dataset'\n",
        "training_volume = 'training'\n",
        "\n",
        "mlflow_experiment = '/Users/brian.law@databricks.com/brian_yolo_training'\n",
        "\n",
        "volume_path = f\"/Volumes/{ds_catalog}/{ds_schema}/{coco_volume}\"\n",
        "training_volume_path = f\"/local_disk0/ultralytics_logging_folder\"\n",
        "image_path = f'{volume_path}/images'\n",
        "annotation_json = f'{volume_path}/annotations.json'\n",
        "\n",
        "# YOLO model to start from (yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt)\n",
        "# or use yolov8n.pt, yolov8s.pt, etc.\n",
        "YOLO_MODEL = 'yolo11n.pt'  # Start with nano for faster training\n",
        "\n",
        "print(f\"Dataset location: {volume_path}\")\n",
        "print(f\"Training outputs: {training_volume_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Hyperparams parameters\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = 640\n",
        "initial_lr = 0.005\n",
        "final_lr = 0.1\n",
        "run_name = 'single_gpu_run'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare YOLO Dataset Configuration\n",
        "\n",
        "YOLO expects a `.yaml` file that defines:\n",
        "- Path to train/val images\n",
        "- Number of classes\n",
        "- Class names\n",
        "\n",
        "We'll convert our COCO format to YOLO's expected structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import yaml\n",
        "\n",
        "# Read COCO annotations to get class information\n",
        "with open(annotation_json, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Extract categories and create mapping from COCO category_id to YOLO class_id (0-indexed)\n",
        "categories = coco_data['categories']\n",
        "sorted_categories = sorted(categories, key=lambda x: x['id'])\n",
        "class_names = [cat['name'] for cat in sorted_categories]\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# CRITICAL: Create mapping from COCO category_id to YOLO class index (0-based)\n",
        "# COCO IDs might be non-contiguous (e.g., 1, 2, 3, ..., 90) but YOLO needs 0, 1, 2, ..., n-1\n",
        "coco_id_to_yolo_id = {cat['id']: idx for idx, cat in enumerate(sorted_categories)}\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Classes: {class_names[:10]}...\")  # Show first 10\n",
        "print(f\"COCO ID to YOLO ID mapping sample: {dict(list(coco_id_to_yolo_id.items())[:5])}\")\n",
        "\n",
        "# Create YOLO dataset config\n",
        "yolo_config = {\n",
        "    'path': volume_path,  # Root directory\n",
        "    'train': 'images',  # Train images relative to 'path'\n",
        "    'val': 'images',    # Using same for now - split in production\n",
        "    'nc': num_classes,  # Number of classes\n",
        "    'names': class_names  # Class names\n",
        "}\n",
        "\n",
        "# Save config file\n",
        "config_path = f\"{volume_path}/data.yaml\"\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.dump(yolo_config, f)\n",
        "\n",
        "print(f\"\\nYOLO config saved to: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert COCO to YOLO Format\n",
        "\n",
        "YOLO expects annotations in a specific format:\n",
        "- One `.txt` file per image\n",
        "- Each line: `class_id center_x center_y width height` (normalized 0-1)\n",
        "\n",
        "We'll convert the COCO annotations to YOLO format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Diagnose COCO Data Format (Run this to debug coordinate issues)\n",
        "\n",
        "# Inspect a few annotations to understand the coordinate format\n",
        "print(\"=== COCO Data Inspection ===\\n\")\n",
        "\n",
        "# Check images\n",
        "sample_images = coco_data['images'][:3]\n",
        "print(f\"Total images: {len(coco_data['images'])}\")\n",
        "print(f\"\\nSample images:\")\n",
        "for img in sample_images:\n",
        "    print(f\"  ID: {img['id']}, File: {img['file_name']}, Size: {img['width']}x{img['height']}\")\n",
        "\n",
        "# Check annotations\n",
        "sample_annotations = coco_data['annotations'][:5]\n",
        "print(f\"\\nTotal annotations: {len(coco_data['annotations'])}\")\n",
        "print(f\"\\nSample annotations:\")\n",
        "for ann in sample_annotations:\n",
        "    img_info = next((img for img in coco_data['images'] if img['id'] == ann['image_id']), None)\n",
        "    if img_info:\n",
        "        bbox = ann['bbox']\n",
        "        print(f\"\\n  Annotation ID: {ann['id']}\")\n",
        "        print(f\"    Image: {img_info['file_name']} ({img_info['width']}x{img_info['height']})\")\n",
        "        print(f\"    Category ID: {ann['category_id']}\")\n",
        "        print(f\"    Bbox: {bbox}\")\n",
        "        print(f\"    Bbox/Image ratio: x={bbox[0]/img_info['width']:.3f}, y={bbox[1]/img_info['height']:.3f}, w={bbox[2]/img_info['width']:.3f}, h={bbox[3]/img_info['height']:.3f}\")\n",
        "        \n",
        "        # Check if coordinates seem normalized or in pixels\n",
        "        if bbox[0] < 2 and bbox[1] < 2 and bbox[2] < 2 and bbox[3] < 2:\n",
        "            print(f\"    ⚠️  WARNING: Bbox values are < 2, might already be normalized!\")\n",
        "        if bbox[0] > img_info['width'] or bbox[1] > img_info['height']:\n",
        "            print(f\"    ⚠️  WARNING: Bbox x/y exceed image dimensions!\")\n",
        "        if (bbox[0] + bbox[2]) > img_info['width'] or (bbox[1] + bbox[3]) > img_info['height']:\n",
        "            print(f\"    ⚠️  WARNING: Bbox extends beyond image boundaries!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create labels directory\n",
        "labels_dir = Path(volume_path) / 'labels'\n",
        "labels_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Converting COCO annotations to YOLO format...\")\n",
        "\n",
        "# Group annotations by image_id\n",
        "image_annotations = {}\n",
        "for ann in coco_data['annotations']:\n",
        "    image_id = ann['image_id']\n",
        "    if image_id not in image_annotations:\n",
        "        image_annotations[image_id] = []\n",
        "    image_annotations[image_id].append(ann)\n",
        "\n",
        "# Convert each image's annotations\n",
        "images_dict = {img['id']: img for img in coco_data['images']}\n",
        "converted_count = 0\n",
        "skipped_annotations = 0\n",
        "invalid_coords_count = 0\n",
        "\n",
        "# Debug: Check first annotation to understand the coordinate format\n",
        "if coco_data['annotations']:\n",
        "    first_ann = coco_data['annotations'][0]\n",
        "    first_img = images_dict[first_ann['image_id']]\n",
        "    print(f\"\\nDebug - First annotation:\")\n",
        "    print(f\"  Image size: {first_img['width']}x{first_img['height']}\")\n",
        "    print(f\"  Bbox (COCO): {first_ann['bbox']}\")\n",
        "    print(f\"  Bbox format should be: [x, y, width, height] in pixels\")\n",
        "\n",
        "for image_id, image_info in images_dict.items():\n",
        "    img_width = image_info['width']\n",
        "    img_height = image_info['height']\n",
        "    \n",
        "    # Get corresponding annotations\n",
        "    annotations = image_annotations.get(image_id, [])\n",
        "    \n",
        "    # Create label file\n",
        "    image_filename = Path(image_info['file_name']).stem\n",
        "    label_file = labels_dir / f\"{image_filename}.txt\"\n",
        "    \n",
        "    with open(label_file, 'w') as f:\n",
        "        for ann in annotations:\n",
        "            # Get COCO category_id and map to YOLO class index\n",
        "            coco_category_id = ann['category_id']\n",
        "            \n",
        "            # Skip if category_id is not in the mapping (should not happen with valid COCO data)\n",
        "            if coco_category_id not in coco_id_to_yolo_id:\n",
        "                print(f\"Warning: Unknown category_id {coco_category_id} in image {image_filename}\")\n",
        "                skipped_annotations += 1\n",
        "                continue\n",
        "            \n",
        "            # Map to YOLO class index (0-based)\n",
        "            yolo_class_id = coco_id_to_yolo_id[coco_category_id]\n",
        "            \n",
        "            # COCO format: [x, y, width, height] (top-left corner)\n",
        "            x, y, w, h = ann['bbox']\n",
        "            \n",
        "            # Validate bbox values are positive and reasonable\n",
        "            if x < 0 or y < 0 or w <= 0 or h <= 0:\n",
        "                if invalid_coords_count == 0:\n",
        "                    print(f\"Warning: Invalid bbox in {image_filename}: x={x}, y={y}, w={w}, h={h}\")\n",
        "                invalid_coords_count += 1\n",
        "                continue\n",
        "            \n",
        "            # Convert to YOLO format: [center_x, center_y, width, height] (normalized)\n",
        "            center_x = (x + w / 2) / img_width\n",
        "            center_y = (y + h / 2) / img_height\n",
        "            norm_w = w / img_width\n",
        "            norm_h = h / img_height\n",
        "            \n",
        "            # Validate normalized coordinates are in valid range [0, 1]\n",
        "            # Allow slight overflow due to floating point, but clip to [0, 1]\n",
        "            if center_x > 1.05 or center_y > 1.05 or norm_w > 1.05 or norm_h > 1.05:\n",
        "                if invalid_coords_count < 5:  # Print first few examples\n",
        "                    print(f\"\\nWarning: Out of bounds coordinates in {image_filename}:\")\n",
        "                    print(f\"  Image size: {img_width}x{img_height}\")\n",
        "                    print(f\"  COCO bbox: x={x}, y={y}, w={w}, h={h}\")\n",
        "                    print(f\"  YOLO (before clip): cx={center_x:.4f}, cy={center_y:.4f}, w={norm_w:.4f}, h={norm_h:.4f}\")\n",
        "                invalid_coords_count += 1\n",
        "                # Skip this annotation if severely out of bounds\n",
        "                if center_x > 1.5 or center_y > 1.5 or norm_w > 1.5 or norm_h > 1.5:\n",
        "                    continue\n",
        "            \n",
        "            # Clip coordinates to valid range [0, 1]\n",
        "            center_x = max(0.0, min(1.0, center_x))\n",
        "            center_y = max(0.0, min(1.0, center_y))\n",
        "            norm_w = max(0.0, min(1.0, norm_w))\n",
        "            norm_h = max(0.0, min(1.0, norm_h))\n",
        "            \n",
        "            # Write in YOLO format (using mapped class_id)\n",
        "            f.write(f\"{yolo_class_id} {center_x:.6f} {center_y:.6f} {norm_w:.6f} {norm_h:.6f}\\n\")\n",
        "    \n",
        "    converted_count += 1\n",
        "    if converted_count % 100 == 0:\n",
        "        print(f\"Converted {converted_count}/{len(images_dict)} images...\")\n",
        "\n",
        "print(f\"\\nConversion complete! {converted_count} label files created in {labels_dir}\")\n",
        "if skipped_annotations > 0:\n",
        "    print(f\"Warning: Skipped {skipped_annotations} annotations with unknown category IDs\")\n",
        "if invalid_coords_count > 0:\n",
        "    print(f\"Warning: Found {invalid_coords_count} annotations with invalid/out-of-bounds coordinates (clipped or skipped)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training on Single GPU\n",
        "\n",
        "Start with single GPU training to validate the setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from ultralytics import settings\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "settings.update({\"mlflow\": True})\n",
        "\n",
        "# Setting MLflow configs for ultralytics\n",
        "os.environ['MLFLOW_EXPERIMENT_NAME'] = mlflow_experiment\n",
        "os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
        "\n",
        "# keep run active to log the best model into mlflow\n",
        "os.environ['MLFLOW_KEEP_RUN_ACTIVE'] = \"true\"\n",
        "\n",
        "# setup torch routines that Ultralytics requires\n",
        "if not dist.is_initialized():\n",
        "    dist.init_process_group(backend=\"nccl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Load pretrained model\n",
        "model = YOLO(YOLO_MODEL)\n",
        "\n",
        "print(f\"\\nLoaded {YOLO_MODEL}\")\n",
        "print(f\"Model summary:\")\n",
        "model.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "results = model.train(\n",
        "        data=config_path,\n",
        "        epochs=EPOCHS,\n",
        "        batch=BATCH_SIZE,\n",
        "        lr0=initial_lr,       # initial learning rate\n",
        "        lrf=final_lr,         # final LR factor (relative to lr0)\n",
        "        imgsz=IMG_SIZE,\n",
        "        name=run_name,\n",
        "        project=training_volume_path,\n",
        "        device=0,  # Use GPU 0\n",
        "        workers=8,\n",
        "        patience=50,\n",
        "        save=True,\n",
        "        save_period=5,  # Save checkpoint every 5 epochs\n",
        "        verbose=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLflow Integration\n",
        "\n",
        "Ultralytics YOLO includes built-in MLflow callback integration for logging training metrics and checkpoints. We extend this with:\n",
        "\n",
        "- **Dataset Tracking**: Log COCO dataset metadata for full lineage\n",
        "- **Deployment-Ready Models**: PyFunc wrapper with base64 input support for Model Serving\n",
        "- **Metrics Linkage**: Connect validation metrics to both model and dataset (MLflow 3.x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log Dataset to Active MLflow Run\n",
        "\n",
        "Use `mlflow.data` module to log the dataset as a trackable input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the active run (should be the training run)\n",
        "active_run = mlflow.active_run()\n",
        "\n",
        "if active_run:\n",
        "    print(f\"Active Run ID: {active_run.info.run_id}\")\n",
        "    print(f\"Logging dataset using mlflow.data module...\\n\")\n",
        "    \n",
        "    # Prepare dataset metadata DataFrame\n",
        "    # Include image info and link to annotations\n",
        "    image_data = []\n",
        "    \n",
        "    # Create mapping of image_id to annotations\n",
        "    image_to_anns = {}\n",
        "    for ann in coco_data['annotations']:\n",
        "        img_id = ann['image_id']\n",
        "        if img_id not in image_to_anns:\n",
        "            image_to_anns[img_id] = []\n",
        "        image_to_anns[img_id].append(ann)\n",
        "    \n",
        "    # Build comprehensive dataset representation\n",
        "    for img in coco_data['images']:\n",
        "        img_id = img['id']\n",
        "        anns = image_to_anns.get(img_id, [])\n",
        "        \n",
        "        # Get class distribution for this image\n",
        "        classes_in_image = [ann['category_id'] for ann in anns]\n",
        "        \n",
        "        image_data.append({\n",
        "            'image_id': img_id,\n",
        "            'file_name': img['file_name'],\n",
        "            'width': img['width'],\n",
        "            'height': img['height'],\n",
        "            'num_annotations': len(anns),\n",
        "            'classes': ','.join(map(str, classes_in_image)) if classes_in_image else ''\n",
        "        })\n",
        "    \n",
        "    dataset_df = pd.DataFrame(image_data)\n",
        "    \n",
        "    # Create MLflow Dataset with full metadata\n",
        "    dataset = mlflow.data.from_pandas(\n",
        "        dataset_df,\n",
        "        source=volume_path,\n",
        "        name=f\"{ds_catalog}.{ds_schema}.{coco_volume}\",\n",
        "        targets=\"num_annotations\",  # What we're predicting\n",
        "    )\n",
        "    \n",
        "    # Log the dataset as an input to the training run\n",
        "    mlflow.log_input(dataset, context=\"training\")\n",
        "    \n",
        "    print(f\"✓ Logged dataset with {len(dataset_df)} images\")\n",
        "    print(f\"  - Source: {volume_path}\")\n",
        "    print(f\"  - Name: {ds_catalog}.{ds_schema}.{coco_volume}\")\n",
        "    print(f\"  - Total annotations: {dataset_df['num_annotations'].sum()}\")\n",
        "    print(f\"  - Avg annotations/image: {dataset_df['num_annotations'].mean():.2f}\")\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Dataset logged to active run!\")\n",
        "    print(f\"View in MLflow UI at: {mlflow_experiment}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠ No active MLflow run found!\")\n",
        "    print(\"The run may have already been ended by Ultralytics.\")\n",
        "    print(\"\\nTo manually log the dataset, use the cell below with a specific run_id.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log YOLO Model for Deployment\n",
        "\n",
        "Create an MLflow PyFunc wrapper that makes the model deployment-ready:\n",
        "\n",
        "- **Handles base64 input**: REST API-friendly image encoding\n",
        "- **Complete preprocessing**: Image decoding, resizing, and normalization\n",
        "- **Post-processing included**: NMS and confidence filtering applied internally\n",
        "- **Structured output**: Returns detection boxes, scores, and class IDs\n",
        "- **Artifact-based loading**: Model weights loaded from MLflow artifacts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PyFunc Model Wrapper for Deployment\n",
        "# This wrapper handles base64 image input and includes the model weights as artifacts\n",
        "import torch\n",
        "from torchvision.ops import nms\n",
        "import mlflow.pyfunc\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class YOLOPyFuncWrapper(mlflow.pyfunc.PythonModel):\n",
        "    \"\"\"\n",
        "    MLflow PyFunc wrapper for YOLO model with base64 string input support.\n",
        "    Handles decoding, preprocessing, inference, and post-processing.\n",
        "    Uses artifacts to load the model file.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, img_size=640, conf_thres=0.25, iou_thres=0.5, max_det=300):\n",
        "        \"\"\"Initialize with detection parameters\"\"\"\n",
        "        self.img_size = img_size\n",
        "        self.conf_thres = conf_thres\n",
        "        self.iou_thres = iou_thres\n",
        "        self.max_det = max_det\n",
        "        self.model = None\n",
        "    \n",
        "    def load_context(self, context):\n",
        "        \"\"\"Load the YOLO model during MLflow model loading\"\"\"\n",
        "        from ultralytics import YOLO\n",
        "        import torch\n",
        "        \n",
        "        # Load the YOLO model from artifacts\n",
        "        model_path = context.artifacts[\"model\"]\n",
        "        yolo = YOLO(model_path)\n",
        "        self.model = yolo.model.eval()\n",
        "        \n",
        "        print(f\"YOLO model loaded from artifacts: {model_path}\")\n",
        "    \n",
        "    @staticmethod\n",
        "    def xywh_to_xyxy(b):\n",
        "        \"\"\"Convert box format from xywh to xyxy\"\"\"\n",
        "        x, y, w, h = b.unbind(-1)\n",
        "        return torch.stack([x-w/2, y-h/2, x+w/2, y+h/2], dim=-1)\n",
        "    \n",
        "    def preprocess_base64(self, base64_str):\n",
        "        \"\"\"Decode base64 image and convert to tensor format\"\"\"\n",
        "        # Handle different string types (str, np.str_, bytes)\n",
        "        if isinstance(base64_str, bytes):\n",
        "            base64_str = base64_str.decode('utf-8')\n",
        "        else:\n",
        "            base64_str = str(base64_str)\n",
        "        \n",
        "        # Decode base64 to bytes\n",
        "        img_bytes = base64.b64decode(base64_str)\n",
        "        \n",
        "        # Load as PIL Image\n",
        "        img = Image.open(BytesIO(img_bytes)).convert('RGB')\n",
        "        \n",
        "        # Resize to expected size\n",
        "        img = img.resize((self.img_size, self.img_size), Image.BILINEAR)\n",
        "        \n",
        "        # Convert to numpy array and normalize\n",
        "        img_array = np.array(img, dtype=np.float32) / 255.0\n",
        "        \n",
        "        # Convert HWC to CHW format\n",
        "        img_array = np.transpose(img_array, (2, 0, 1))\n",
        "        \n",
        "        # Convert to torch tensor with batch dimension\n",
        "        img_tensor = torch.from_numpy(img_array).unsqueeze(0)\n",
        "        \n",
        "        return img_tensor\n",
        "    \n",
        "    def predict(self, context, model_input):\n",
        "        \"\"\"\n",
        "        Handle prediction requests from Model Serving endpoint.\n",
        "        \n",
        "        Args:\n",
        "            context: MLflow context (unused in predict, used in load_context)\n",
        "            model_input: Dict, DataFrame, or direct input with 'images' containing base64 strings\n",
        "            \n",
        "        Returns:\n",
        "            numpy array of detections with shape (batch_size, max_det, 6)\n",
        "            Each detection: [x1, y1, x2, y2, confidence, class_id]\n",
        "        \"\"\"\n",
        "        # Extract images from input\n",
        "        if isinstance(model_input, dict):\n",
        "            images = model_input.get('images')\n",
        "        elif hasattr(model_input, 'to_dict'):  # DataFrame\n",
        "            images = model_input['images'].tolist() if 'images' in model_input.columns else None\n",
        "            if images and len(images) == 1:\n",
        "                images = images[0]\n",
        "        elif hasattr(model_input, '__getitem__'):  # List or array-like\n",
        "            images = model_input\n",
        "        else:\n",
        "            images = str(model_input)\n",
        "        \n",
        "        if images is None:\n",
        "            raise ValueError(\"Input must contain 'images' key with base64 encoded image string\")\n",
        "        \n",
        "        # Handle different input formats for base64 strings\n",
        "        with torch.no_grad():\n",
        "            # Single string (most common case for serving endpoint)\n",
        "            if isinstance(images, (str, bytes, np.str_)):\n",
        "                x = self.preprocess_base64(images)\n",
        "            \n",
        "            # 0-dimensional numpy array containing a string\n",
        "            elif isinstance(images, np.ndarray) and images.ndim == 0:\n",
        "                x = self.preprocess_base64(images.item())\n",
        "            \n",
        "            # List/tuple of strings (batch)\n",
        "            elif isinstance(images, (list, tuple)):\n",
        "                processed = [self.preprocess_base64(img) for img in images]\n",
        "                x = torch.cat(processed, dim=0)\n",
        "            \n",
        "            # Numpy array of strings\n",
        "            elif isinstance(images, np.ndarray) and images.dtype.kind in ('U', 'S', 'O'):\n",
        "                processed = [self.preprocess_base64(str(img)) for img in images.flat]\n",
        "                x = torch.cat(processed, dim=0)\n",
        "            \n",
        "            else:\n",
        "                raise ValueError(\n",
        "                    f\"Unsupported input type: {type(images)}. \"\n",
        "                    f\"Expected base64 encoded string or list of strings. \"\n",
        "                    f\"Input dtype: {getattr(images, 'dtype', 'N/A')}\"\n",
        "                )\n",
        "            \n",
        "            # Move to model device\n",
        "            device = next(self.model.parameters()).device\n",
        "            x = x.to(device)\n",
        "            \n",
        "            # Run inference\n",
        "            out = self.model(x)\n",
        "            \n",
        "            # Handle different output formats\n",
        "            if isinstance(out, (list, tuple)):\n",
        "                out = out[0]\n",
        "            \n",
        "            # Ensure correct dimension ordering\n",
        "            if out.dim() == 3 and out.shape[1] < out.shape[2]:\n",
        "                out = out.permute(0, 2, 1).contiguous()\n",
        "            \n",
        "            # Parse detections: [batch, anchors, 4+1+num_classes]\n",
        "            boxes_xywh = out[..., :4]\n",
        "            obj = out[..., 4:5].sigmoid()\n",
        "            cls = out[..., 5:].sigmoid()\n",
        "            \n",
        "            # Compute confidence scores and class IDs\n",
        "            conf, cls_id = (obj * cls).max(-1)\n",
        "            \n",
        "            # Convert boxes to xyxy format\n",
        "            boxes = self.xywh_to_xyxy(boxes_xywh)\n",
        "            \n",
        "            # Apply NMS and filtering per image\n",
        "            N = boxes.shape[0]\n",
        "            out_pad = x.new_full((N, self.max_det, 6), -1.0)\n",
        "            \n",
        "            for i in range(N):\n",
        "                # Filter by confidence threshold\n",
        "                mask = conf[i] >= self.conf_thres\n",
        "                if mask.sum() == 0:\n",
        "                    continue\n",
        "                \n",
        "                b = boxes[i][mask]\n",
        "                s = conf[i][mask]\n",
        "                c = cls_id[i][mask].float()\n",
        "                \n",
        "                # Apply NMS\n",
        "                keep = nms(b, s, self.iou_thres)[:self.max_det]\n",
        "                \n",
        "                # Store results\n",
        "                k = keep.numel()\n",
        "                if k > 0:\n",
        "                    out_pad[i, :k, :4] = b[keep]\n",
        "                    out_pad[i, :k, 4] = s[keep]\n",
        "                    out_pad[i, :k, 5] = c[keep]\n",
        "            \n",
        "            # Convert to numpy\n",
        "            result = out_pad.cpu().numpy()\n",
        "            \n",
        "            return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare best model path and artifacts for pyfunc wrapper\n",
        "import tempfile\n",
        "import shutil\n",
        "\n",
        "best_model_path = f\"{training_volume_path}/{run_name}/weights/best.pt\"\n",
        "\n",
        "# Create temporary directory for artifacts\n",
        "artifacts_dir = tempfile.mkdtemp()\n",
        "\n",
        "# Copy the best.pt file to artifacts directory\n",
        "artifact_model_path = os.path.join(artifacts_dir, \"best.pt\")\n",
        "shutil.copy2(best_model_path, artifact_model_path)\n",
        "\n",
        "# Create artifacts dict\n",
        "artifacts = {\n",
        "    \"model\": artifact_model_path\n",
        "}\n",
        "\n",
        "# Create pyfunc wrapper instance\n",
        "pyfunc_wrapper = YOLOPyFuncWrapper(\n",
        "    img_size=IMG_SIZE,\n",
        "    conf_thres=0.25,\n",
        "    iou_thres=0.5,\n",
        "    max_det=300\n",
        ")\n",
        "\n",
        "print(f\"✓ Prepared model artifacts from: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflow.models.signature import ModelSignature\n",
        "from mlflow.types import Schema, TensorSpec, ColSpec\n",
        "import numpy as np\n",
        "\n",
        "# Create signature with base64 string input (deployment-ready)\n",
        "signature = ModelSignature(\n",
        "    inputs=Schema([ColSpec(\"string\", \"images\")]),\n",
        "    outputs=Schema([TensorSpec(np.dtype(np.float32), (-1, -1, 6), \"detections\")])\n",
        ")\n",
        "\n",
        "# Create input example as base64 encoded image\n",
        "dummy_img = np.random.randint(0, 255, (IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8)\n",
        "pil_img = Image.fromarray(dummy_img)\n",
        "buffer = BytesIO()\n",
        "pil_img.save(buffer, format='PNG')\n",
        "img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "input_example = {\"images\": img_base64}\n",
        "\n",
        "# Log model as pyfunc with artifacts\n",
        "try:\n",
        "    model_info = mlflow.pyfunc.log_model(\n",
        "        python_model=pyfunc_wrapper,\n",
        "        artifact_path=\"best_model\",\n",
        "        artifacts=artifacts,\n",
        "        signature=signature,\n",
        "        input_example=input_example,\n",
        "        pip_requirements=[\n",
        "            \"torch\",\n",
        "            \"torchvision\", \n",
        "            \"ultralytics\",\n",
        "            \"pillow\",\n",
        "            \"numpy\"\n",
        "        ]\n",
        "        #registered_model_name=\"brian_ml_dev.image_processing.yolo_best_model\"  # optional: register to UC\n",
        "    )\n",
        "    \n",
        "    model_id = model_info.model_id\n",
        "    print(f\"✓ Model logged successfully!\")\n",
        "    print(f\"  Model ID: {model_id}\")\n",
        "    print(f\"  Artifact path: best_model\")\n",
        "    print(f\"  Input format: base64 encoded image strings\")\n",
        "    print(f\"  Output format: [batch, detections, 6] where each detection is [x1, y1, x2, y2, conf, class_id]\")\n",
        "    \n",
        "finally:\n",
        "    # Clean up temporary directory\n",
        "    shutil.rmtree(artifacts_dir, ignore_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log Final Metrics with Model and Dataset Linkage\n",
        "\n",
        "Using MLflow 3.x features, we can link metrics to both the logged model and dataset for complete lineage tracking.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Log final validation metrics linked to both LoggedModel and Dataset\n",
        "try:\n",
        "    # Get the dataset entity we logged earlier\n",
        "    active_run = mlflow.active_run()\n",
        "    \n",
        "    if active_run and 'dataset' in locals():\n",
        "        print(f\"Logging final metrics linked to model and dataset...\")\n",
        "        \n",
        "        # Extract final validation metrics from training results\n",
        "        # Note: In single GPU, we need to access the results object\n",
        "        # The results object should still be available from the training cell\n",
        "        \n",
        "        # Get validation metrics (these are typically available in the results object)\n",
        "        # For demonstration, we'll compute from the training results\n",
        "        val_mAP50 = 0.0  # Placeholder - extract from results if available\n",
        "        val_mAP50_95 = 0.0  # Placeholder - extract from results if available\n",
        "        \n",
        "        # Try to extract from results if available\n",
        "        if 'results' in locals() and hasattr(results, 'results_dict'):\n",
        "            val_mAP50 = float(results.results_dict.get('metrics/mAP50(B)', 0.0))\n",
        "            val_mAP50_95 = float(results.results_dict.get('metrics/mAP50-95(B)', 0.0))\n",
        "        \n",
        "        # Log metrics linked to both model_id and dataset\n",
        "        mlflow.log_metric(\n",
        "            key=\"final_val_mAP50\",\n",
        "            value=val_mAP50,\n",
        "            step=EPOCHS,\n",
        "            model_id=model_id,  # Links to LoggedModel\n",
        "            dataset=dataset  # Links to dataset\n",
        "        )\n",
        "        \n",
        "        mlflow.log_metric(\n",
        "            key=\"final_val_mAP50-95\",\n",
        "            value=val_mAP50_95,\n",
        "            step=EPOCHS,\n",
        "            model_id=model_id,  # Links to LoggedModel\n",
        "            dataset=dataset  # Links to dataset\n",
        "        )\n",
        "        \n",
        "        print(f\"✓ Final metrics logged and linked!\")\n",
        "        print(f\"  - mAP50: {val_mAP50:.4f}\")\n",
        "        print(f\"  - mAP50-95: {val_mAP50_95:.4f}\")\n",
        "        print(f\"  - Linked to model_id: {model_id}\")\n",
        "        print(f\"  - Linked to dataset: {ds_catalog}.{ds_schema}.{coco_volume}\")\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(\"Complete lineage established:\")\n",
        "        print(\"  Dataset → Training → Model → Metrics\")\n",
        "        print(f\"{'='*60}\")\n",
        "    else:\n",
        "        print(\"⚠ Dataset not available for linking. Make sure Cell 17 was executed.\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Warning: Could not log final metrics with linkage - {e}\")\n",
        "    print(\"This is expected if using MLflow < 3.0 or if dataset was not logged\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close out active mlflow run\n",
        "mlflow.end_run()\n",
        "print(\"✓ MLflow run ended successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next Steps\n",
        "\n",
        "## What You've Created\n",
        "\n",
        "✅ **Complete Lineage Tracking:**\n",
        "- Dataset logged with COCO metadata and image statistics\n",
        "- Training hyperparameters captured automatically by Ultralytics\n",
        "- Model logged with deployment-ready PyFunc wrapper\n",
        "- Final metrics linked to both model and dataset (MLflow 3.x)\n",
        "\n",
        "✅ **Deployment-Ready Model:**\n",
        "- Accepts base64 encoded image strings (REST API friendly)\n",
        "- Handles preprocessing and post-processing internally\n",
        "- Returns structured detection output: `[batch, detections, 6]` where each detection is `[x1, y1, x2, y2, confidence, class_id]`\n",
        "- Includes all dependencies in pip_requirements\n",
        "\n",
        "## Deployment Options\n",
        "\n",
        "### 1. Model Serving Endpoint\n",
        "Deploy the logged model to a Databricks Model Serving endpoint:\n",
        "```python\n",
        "# Get the model URI from the MLflow run\n",
        "model_uri = f\"runs:/{run_id}/best_model\"\n",
        "\n",
        "# Create serving endpoint via Databricks UI or API\n",
        "```\n",
        "\n",
        "### 2. Register to Unity Catalog\n",
        "For versioned model management, uncomment `registered_model_name` in Cell 21:\n",
        "```python\n",
        "registered_model_name=\"<catalog>.<schema>.<model_name>\"\n",
        "```\n",
        "\n",
        "### 3. Batch Inference\n",
        "Load the model for batch processing on Spark:\n",
        "```python\n",
        "model = mlflow.pyfunc.load_model(model_uri)\n",
        "predictions = model.predict({\"images\": base64_images})\n",
        "```\n",
        "\n",
        "## Training at Scale\n",
        "\n",
        "For larger datasets or faster training, use the multi-GPU notebook which leverages TorchDistributor with the `scripts/train_yolo.py` training script across multiple GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
