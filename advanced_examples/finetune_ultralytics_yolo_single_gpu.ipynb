{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Finetuning with Ultralytics YOLO\n",
        "\n",
        "This notebook demonstrates fine-tuning the latest Ultralytics YOLO models (YOLO11/YOLOv8) on custom datasets using single GPU training.\n",
        "\n",
        "**Key Advantages over DETR:**\n",
        "- Simpler API and less boilerplate code\n",
        "- Faster training and inference\n",
        "- Native COCO format support\n",
        "- Better production-ready tooling\n",
        "- Straightforward single GPU training\n",
        "\n",
        "**References:**\n",
        "- https://docs.ultralytics.com/modes/train/\n",
        "- https://docs.ultralytics.com/datasets/detect/\n",
        "\n",
        "**Prerequesites**\n",
        "- MLR 17.3 LTS - need numpy 2.z compatability\n",
        "- Cluster with GPU\n",
        "- Cluster started with `scripts/init_script_ultralytics.sh` init script - this will install ultralytics on the base env before starting it to avoid version clashes if we were to just pip install in the notbook or via the usual cluster config\n",
        "- This notebook assumse that you have run at least: \n",
        "  - The `Precessing_w_ffmpeg` notebook to standardise a set of input videos\n",
        "  - The `2_Batch_inference_w_opencv` notebook to create a coco format dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sh\n",
        "# Make sure to install these via init scripts for sustainability\n",
        "# /databricks/python/bin/pip install -U ultralytics supervision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U mlflow psutil nvidia-ml-py\n",
        "%restart_python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup and Configure\n",
        "\n",
        "Initialize all variables needed for training. We'll use the same UC Volumes structure as the DETR notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "ds_catalog = 'brian_ml_dev'\n",
        "ds_schema = 'image_processing'\n",
        "coco_volume = 'coco_dataset'\n",
        "training_volume = 'training'\n",
        "\n",
        "mlflow_experiment = '/Users/brian.law@databricks.com/brian_yolo_training'\n",
        "\n",
        "volume_path = f\"/Volumes/{ds_catalog}/{ds_schema}/{coco_volume}\"\n",
        "training_volume_path = f\"/local_disk0/ultralytics_logging_folder\"\n",
        "image_path = f'{volume_path}/images'\n",
        "annotation_json = f'{volume_path}/annotations.json'\n",
        "\n",
        "# YOLO model to start from (yolo11n.pt, yolo11s.pt, yolo11m.pt, yolo11l.pt, yolo11x.pt)\n",
        "# or use yolov8n.pt, yolov8s.pt, etc.\n",
        "YOLO_MODEL = 'yolo11n.pt'  # Start with nano for faster training\n",
        "\n",
        "print(f\"Dataset location: {volume_path}\")\n",
        "print(f\"Training outputs: {training_volume_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Hyperparams parameters\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = 640\n",
        "initial_lr = 0.005\n",
        "final_lr = 0.1\n",
        "run_name = 'single_gpu_run'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare YOLO Dataset Configuration\n",
        "\n",
        "YOLO expects a `.yaml` file that defines:\n",
        "- Path to train/val images\n",
        "- Number of classes\n",
        "- Class names\n",
        "\n",
        "We'll convert our COCO format to YOLO's expected structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import yaml\n",
        "\n",
        "# Read COCO annotations to get class information\n",
        "with open(annotation_json, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Extract categories and create mapping from COCO category_id to YOLO class_id (0-indexed)\n",
        "categories = coco_data['categories']\n",
        "sorted_categories = sorted(categories, key=lambda x: x['id'])\n",
        "class_names = [cat['name'] for cat in sorted_categories]\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# CRITICAL: Create mapping from COCO category_id to YOLO class index (0-based)\n",
        "# COCO IDs might be non-contiguous (e.g., 1, 2, 3, ..., 90) but YOLO needs 0, 1, 2, ..., n-1\n",
        "coco_id_to_yolo_id = {cat['id']: idx for idx, cat in enumerate(sorted_categories)}\n",
        "\n",
        "print(f\"Number of classes: {num_classes}\")\n",
        "print(f\"Classes: {class_names[:10]}...\")  # Show first 10\n",
        "print(f\"COCO ID to YOLO ID mapping sample: {dict(list(coco_id_to_yolo_id.items())[:5])}\")\n",
        "\n",
        "# Create YOLO dataset config\n",
        "yolo_config = {\n",
        "    'path': volume_path,  # Root directory\n",
        "    'train': 'images',  # Train images relative to 'path'\n",
        "    'val': 'images',    # Using same for now - split in production\n",
        "    'nc': num_classes,  # Number of classes\n",
        "    'names': class_names  # Class names\n",
        "}\n",
        "\n",
        "# Save config file\n",
        "config_path = f\"{volume_path}/data.yaml\"\n",
        "with open(config_path, 'w') as f:\n",
        "    yaml.dump(yolo_config, f)\n",
        "\n",
        "print(f\"\\nYOLO config saved to: {config_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert COCO to YOLO Format\n",
        "\n",
        "YOLO expects annotations in a specific format:\n",
        "- One `.txt` file per image\n",
        "- Each line: `class_id center_x center_y width height` (normalized 0-1)\n",
        "\n",
        "We'll convert the COCO annotations to YOLO format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Diagnose COCO Data Format (Run this to debug coordinate issues)\n",
        "\n",
        "# Inspect a few annotations to understand the coordinate format\n",
        "print(\"=== COCO Data Inspection ===\\n\")\n",
        "\n",
        "# Check images\n",
        "sample_images = coco_data['images'][:3]\n",
        "print(f\"Total images: {len(coco_data['images'])}\")\n",
        "print(f\"\\nSample images:\")\n",
        "for img in sample_images:\n",
        "    print(f\"  ID: {img['id']}, File: {img['file_name']}, Size: {img['width']}x{img['height']}\")\n",
        "\n",
        "# Check annotations\n",
        "sample_annotations = coco_data['annotations'][:5]\n",
        "print(f\"\\nTotal annotations: {len(coco_data['annotations'])}\")\n",
        "print(f\"\\nSample annotations:\")\n",
        "for ann in sample_annotations:\n",
        "    img_info = next((img for img in coco_data['images'] if img['id'] == ann['image_id']), None)\n",
        "    if img_info:\n",
        "        bbox = ann['bbox']\n",
        "        print(f\"\\n  Annotation ID: {ann['id']}\")\n",
        "        print(f\"    Image: {img_info['file_name']} ({img_info['width']}x{img_info['height']})\")\n",
        "        print(f\"    Category ID: {ann['category_id']}\")\n",
        "        print(f\"    Bbox: {bbox}\")\n",
        "        print(f\"    Bbox/Image ratio: x={bbox[0]/img_info['width']:.3f}, y={bbox[1]/img_info['height']:.3f}, w={bbox[2]/img_info['width']:.3f}, h={bbox[3]/img_info['height']:.3f}\")\n",
        "        \n",
        "        # Check if coordinates seem normalized or in pixels\n",
        "        if bbox[0] < 2 and bbox[1] < 2 and bbox[2] < 2 and bbox[3] < 2:\n",
        "            print(f\"    ⚠️  WARNING: Bbox values are < 2, might already be normalized!\")\n",
        "        if bbox[0] > img_info['width'] or bbox[1] > img_info['height']:\n",
        "            print(f\"    ⚠️  WARNING: Bbox x/y exceed image dimensions!\")\n",
        "        if (bbox[0] + bbox[2]) > img_info['width'] or (bbox[1] + bbox[3]) > img_info['height']:\n",
        "            print(f\"    ⚠️  WARNING: Bbox extends beyond image boundaries!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create labels directory\n",
        "labels_dir = Path(volume_path) / 'labels'\n",
        "labels_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Converting COCO annotations to YOLO format...\")\n",
        "\n",
        "# Group annotations by image_id\n",
        "image_annotations = {}\n",
        "for ann in coco_data['annotations']:\n",
        "    image_id = ann['image_id']\n",
        "    if image_id not in image_annotations:\n",
        "        image_annotations[image_id] = []\n",
        "    image_annotations[image_id].append(ann)\n",
        "\n",
        "# Convert each image's annotations\n",
        "images_dict = {img['id']: img for img in coco_data['images']}\n",
        "converted_count = 0\n",
        "skipped_annotations = 0\n",
        "invalid_coords_count = 0\n",
        "\n",
        "# Debug: Check first annotation to understand the coordinate format\n",
        "if coco_data['annotations']:\n",
        "    first_ann = coco_data['annotations'][0]\n",
        "    first_img = images_dict[first_ann['image_id']]\n",
        "    print(f\"\\nDebug - First annotation:\")\n",
        "    print(f\"  Image size: {first_img['width']}x{first_img['height']}\")\n",
        "    print(f\"  Bbox (COCO): {first_ann['bbox']}\")\n",
        "    print(f\"  Bbox format should be: [x, y, width, height] in pixels\")\n",
        "\n",
        "for image_id, image_info in images_dict.items():\n",
        "    img_width = image_info['width']\n",
        "    img_height = image_info['height']\n",
        "    \n",
        "    # Get corresponding annotations\n",
        "    annotations = image_annotations.get(image_id, [])\n",
        "    \n",
        "    # Create label file\n",
        "    image_filename = Path(image_info['file_name']).stem\n",
        "    label_file = labels_dir / f\"{image_filename}.txt\"\n",
        "    \n",
        "    with open(label_file, 'w') as f:\n",
        "        for ann in annotations:\n",
        "            # Get COCO category_id and map to YOLO class index\n",
        "            coco_category_id = ann['category_id']\n",
        "            \n",
        "            # Skip if category_id is not in the mapping (should not happen with valid COCO data)\n",
        "            if coco_category_id not in coco_id_to_yolo_id:\n",
        "                print(f\"Warning: Unknown category_id {coco_category_id} in image {image_filename}\")\n",
        "                skipped_annotations += 1\n",
        "                continue\n",
        "            \n",
        "            # Map to YOLO class index (0-based)\n",
        "            yolo_class_id = coco_id_to_yolo_id[coco_category_id]\n",
        "            \n",
        "            # COCO format: [x, y, width, height] (top-left corner)\n",
        "            x, y, w, h = ann['bbox']\n",
        "            \n",
        "            # Validate bbox values are positive and reasonable\n",
        "            if x < 0 or y < 0 or w <= 0 or h <= 0:\n",
        "                if invalid_coords_count == 0:\n",
        "                    print(f\"Warning: Invalid bbox in {image_filename}: x={x}, y={y}, w={w}, h={h}\")\n",
        "                invalid_coords_count += 1\n",
        "                continue\n",
        "            \n",
        "            # Convert to YOLO format: [center_x, center_y, width, height] (normalized)\n",
        "            center_x = (x + w / 2) / img_width\n",
        "            center_y = (y + h / 2) / img_height\n",
        "            norm_w = w / img_width\n",
        "            norm_h = h / img_height\n",
        "            \n",
        "            # Validate normalized coordinates are in valid range [0, 1]\n",
        "            # Allow slight overflow due to floating point, but clip to [0, 1]\n",
        "            if center_x > 1.05 or center_y > 1.05 or norm_w > 1.05 or norm_h > 1.05:\n",
        "                if invalid_coords_count < 5:  # Print first few examples\n",
        "                    print(f\"\\nWarning: Out of bounds coordinates in {image_filename}:\")\n",
        "                    print(f\"  Image size: {img_width}x{img_height}\")\n",
        "                    print(f\"  COCO bbox: x={x}, y={y}, w={w}, h={h}\")\n",
        "                    print(f\"  YOLO (before clip): cx={center_x:.4f}, cy={center_y:.4f}, w={norm_w:.4f}, h={norm_h:.4f}\")\n",
        "                invalid_coords_count += 1\n",
        "                # Skip this annotation if severely out of bounds\n",
        "                if center_x > 1.5 or center_y > 1.5 or norm_w > 1.5 or norm_h > 1.5:\n",
        "                    continue\n",
        "            \n",
        "            # Clip coordinates to valid range [0, 1]\n",
        "            center_x = max(0.0, min(1.0, center_x))\n",
        "            center_y = max(0.0, min(1.0, center_y))\n",
        "            norm_w = max(0.0, min(1.0, norm_w))\n",
        "            norm_h = max(0.0, min(1.0, norm_h))\n",
        "            \n",
        "            # Write in YOLO format (using mapped class_id)\n",
        "            f.write(f\"{yolo_class_id} {center_x:.6f} {center_y:.6f} {norm_w:.6f} {norm_h:.6f}\\n\")\n",
        "    \n",
        "    converted_count += 1\n",
        "    if converted_count % 100 == 0:\n",
        "        print(f\"Converted {converted_count}/{len(images_dict)} images...\")\n",
        "\n",
        "print(f\"\\nConversion complete! {converted_count} label files created in {labels_dir}\")\n",
        "if skipped_annotations > 0:\n",
        "    print(f\"Warning: Skipped {skipped_annotations} annotations with unknown category IDs\")\n",
        "if invalid_coords_count > 0:\n",
        "    print(f\"Warning: Found {invalid_coords_count} annotations with invalid/out-of-bounds coordinates (clipped or skipped)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training on Single GPU\n",
        "\n",
        "Start with single GPU training to validate the setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from ultralytics import settings\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "settings.update({\"mlflow\": True})\n",
        "\n",
        "# Setting MLflow configs for ultralytics\n",
        "os.environ['MLFLOW_EXPERIMENT_NAME'] = mlflow_experiment\n",
        "os.environ['MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING'] = \"true\"\n",
        "\n",
        "# keep run active to log the best model into mlflow\n",
        "os.environ['MLFLOW_KEEP_RUN_ACTIVE'] = \"true\"\n",
        "\n",
        "# setup torch routines that Ultralytics requires\n",
        "if not dist.is_initialized():\n",
        "    dist.init_process_group(backend=\"nccl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Load pretrained model\n",
        "model = YOLO(YOLO_MODEL)\n",
        "\n",
        "print(f\"\\nLoaded {YOLO_MODEL}\")\n",
        "print(f\"Model summary:\")\n",
        "model.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "results = model.train(\n",
        "        data=config_path,\n",
        "        epochs=EPOCHS,\n",
        "        batch=BATCH_SIZE,\n",
        "        lr0=initial_lr,       # initial learning rate\n",
        "        lrf=final_lr,         # final LR factor (relative to lr0)\n",
        "        imgsz=IMG_SIZE,\n",
        "        name=run_name,\n",
        "        project=training_volume_path,\n",
        "        device=0,  # Use GPU 0\n",
        "        workers=8,\n",
        "        patience=50,\n",
        "        save=True,\n",
        "        save_period=5,  # Save checkpoint every 5 epochs\n",
        "        verbose=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLflow Integration\n",
        "\n",
        "With the Ultralytics Yolo Framework, there is a default mlflow callback integration. Rather than rewrite this we will build on top of it. \n",
        "\n",
        "Things that we can add include: \n",
        "- Dataset Tracking and Lineage\n",
        "- Proper Model logging to simplify deployment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log Dataset to Active MLflow Run\n",
        "\n",
        "Use `mlflow.data` module to log the dataset as a trackable input.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Get the active run (should be the training run)\n",
        "active_run = mlflow.active_run()\n",
        "\n",
        "if active_run:\n",
        "    print(f\"Active Run ID: {active_run.info.run_id}\")\n",
        "    print(f\"Logging dataset using mlflow.data module...\\n\")\n",
        "    \n",
        "    # Prepare dataset metadata DataFrame\n",
        "    # Include image info and link to annotations\n",
        "    image_data = []\n",
        "    \n",
        "    # Create mapping of image_id to annotations\n",
        "    image_to_anns = {}\n",
        "    for ann in coco_data['annotations']:\n",
        "        img_id = ann['image_id']\n",
        "        if img_id not in image_to_anns:\n",
        "            image_to_anns[img_id] = []\n",
        "        image_to_anns[img_id].append(ann)\n",
        "    \n",
        "    # Build comprehensive dataset representation\n",
        "    for img in coco_data['images']:\n",
        "        img_id = img['id']\n",
        "        anns = image_to_anns.get(img_id, [])\n",
        "        \n",
        "        # Get class distribution for this image\n",
        "        classes_in_image = [ann['category_id'] for ann in anns]\n",
        "        \n",
        "        image_data.append({\n",
        "            'image_id': img_id,\n",
        "            'file_name': img['file_name'],\n",
        "            'width': img['width'],\n",
        "            'height': img['height'],\n",
        "            'num_annotations': len(anns),\n",
        "            'classes': ','.join(map(str, classes_in_image)) if classes_in_image else ''\n",
        "        })\n",
        "    \n",
        "    dataset_df = pd.DataFrame(image_data)\n",
        "    \n",
        "    # Create MLflow Dataset with full metadata\n",
        "    dataset = mlflow.data.from_pandas(\n",
        "        dataset_df,\n",
        "        source=volume_path,\n",
        "        name=f\"{ds_catalog}.{ds_schema}.{coco_volume}\",\n",
        "        targets=\"num_annotations\",  # What we're predicting\n",
        "    )\n",
        "    \n",
        "    # Log the dataset as an input to the training run\n",
        "    mlflow.log_input(dataset, context=\"training\")\n",
        "    \n",
        "    print(f\"✓ Logged dataset with {len(dataset_df)} images\")\n",
        "    print(f\"  - Source: {volume_path}\")\n",
        "    print(f\"  - Name: {ds_catalog}.{ds_schema}.{coco_volume}\")\n",
        "    print(f\"  - Total annotations: {dataset_df['num_annotations'].sum()}\")\n",
        "    print(f\"  - Avg annotations/image: {dataset_df['num_annotations'].mean():.2f}\")\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Dataset logged to active run!\")\n",
        "    print(f\"View in MLflow UI at: {mlflow_experiment}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "else:\n",
        "    print(\"⚠ No active MLflow run found!\")\n",
        "    print(\"The run may have already been ended by Ultralytics.\")\n",
        "    print(\"\\nTo manually log the dataset, use the cell below with a specific run_id.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log Yolo Model as MLflow object\n",
        "\n",
        "The raw pytorch weights are not suitable for easy deployment with databricks model serving or the standard mlflow model format.\n",
        "We can setup a raw pytorch nn module but the Ultralytics Model outputs raw head logits not standard coordinate, score and class formats as expected in object detection.\n",
        "\n",
        "We will create a special torch nn module wrapper that will fix this for us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Raw Model Wrapper definiton\n",
        "import torch, torch.nn as nn\n",
        "from torchvision.ops import nms\n",
        "\n",
        "class YoloDetWrapper(nn.Module):\n",
        "    def __init__(self, base: nn.Module, conf_thres=0.25, iou_thres=0.5, max_det=300):\n",
        "        super().__init__()\n",
        "        self.base = base.eval()\n",
        "        self.conf_thres, self.iou_thres, self.max_det = conf_thres, iou_thres, max_det\n",
        "\n",
        "    @staticmethod\n",
        "    def xywh_to_xyxy(b):\n",
        "        x,y,w,h = b.unbind(-1)\n",
        "        x1 = x - w/2; y1 = y - h/2; x2 = x + w/2; y2 = y + h/2\n",
        "        return torch.stack([x1,y1,x2,y2], dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        with torch.no_grad():\n",
        "            out = self.base(x)                         # e.g. [N, C, A]\n",
        "            if isinstance(out, (list, tuple)): out = out[0]\n",
        "            if out.dim() == 3 and out.shape[1] < out.shape[2]:\n",
        "                out = out.permute(0, 2, 1).contiguous()  # [N, A, C]\n",
        "            # split: [x,y,w,h] + obj + num_classes\n",
        "            boxes_xywh = out[..., :4]\n",
        "            obj = out[..., 4:5].sigmoid()\n",
        "            cls = out[..., 5:].sigmoid()                # shape [N, A, nc]\n",
        "            conf, cls_id = (obj * cls).max(-1)          # [N, A]\n",
        "            boxes = self.xywh_to_xyxy(boxes_xywh)       # [N, A, 4]\n",
        "\n",
        "            N, A = conf.shape\n",
        "            K = self.max_det\n",
        "            out_pad = x.new_full((N, K, 6), -1.0)       # pad with -1\n",
        "\n",
        "            for i in range(N):\n",
        "                mask = conf[i] >= self.conf_thres\n",
        "                if mask.sum() == 0: continue\n",
        "                b = boxes[i][mask]\n",
        "                s = conf[i][mask]\n",
        "                c = cls_id[i][mask].float()\n",
        "                keep = nms(b, s, self.iou_thres)[:K]\n",
        "                k = keep.numel()\n",
        "                if k == 0: continue\n",
        "                out_pad[i, :k, :4] = b[keep]\n",
        "                out_pad[i, :k, 4]  = s[keep]\n",
        "                out_pad[i, :k, 5]  = c[keep]\n",
        "            return out_pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load best YOLO, wrap\n",
        "best_model_raw = YOLO(f\"{training_volume_path}/{run_name}/weights/best.pt\").model\n",
        "wrapped_model = YoloDetWrapper(best_model_raw, conf_thres=0.25, iou_thres=0.5, max_det=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from mlflow.models.signature import ModelSignature\n",
        "from mlflow.types import Schema, TensorSpec\n",
        "import numpy as np\n",
        "\n",
        "# Input: batch of images as float tensors [N,3,H,W]\n",
        "input_schema = Schema([\n",
        "    TensorSpec(type=np.dtype(np.float32), shape=(-1, 3, 640, 640), name=\"images\")\n",
        "])\n",
        "\n",
        "# Output: detections per image [N, num_det, 6]\n",
        "# (cx, cy, w, h, confidence, class)\n",
        "output_schema = Schema([\n",
        "    TensorSpec(type=np.dtype(np.float32), shape=(-1, None, 6), name=\"detections\")\n",
        "])\n",
        "\n",
        "signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
        "\n",
        "mlflow.pytorch.log_model(\n",
        "        pytorch_model=wrapped_model,\n",
        "        name=\"best_model\",\n",
        "        signature=signature,\n",
        "        #registered_model_name=\"yolo_best_model\"  # optional: register to UC model registry\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close out active mlflow run\n",
        "mlflow.end_run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next Steps\n",
        "\n",
        "With the mlflow object that we created above, we now have full lineage on the training dataset plus a wrapped model for deployment.\n",
        "\n",
        "If we comment out the `registered_model_name` and include a full `<catalog>.<schema>.model_name` that will include the model registration.\n",
        "Typically when running a lot of experiments, we may skip this so that we don't end up with a lot of superfluous model versions in the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
