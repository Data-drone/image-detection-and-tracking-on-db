{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLO Hyperparameter Tuning Job\n",
        "\n",
        "This notebook creates a Databricks job to run the YOLO multi-GPU training notebook with hyperparameter optimization using Optuna.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters\n",
        "\n",
        "Define job configuration parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U databricks-sdk optuna opencv-python\n",
        "%restart_python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_name = \"finetune_yolo_job\"\n",
        "notebook_path = \"/Workspace/Users/brian.law@databricks.com/.bundle/image-detection-and-tracking-on-db/dev/files/advanced_examples/finetune_ultralytics_yolo_multi_gpu\"\n",
        "init_script_path = \"/Workspace/Users/brian.law@databricks.com/.bundle/image-detection-and-tracking-on-db/dev/files/scripts/init_script_ultralytics.sh\"\n",
        "max_concurrent_runs = 2\n",
        "\n",
        "# Metrics output location (must match notebook's logging_vol_path)\n",
        "metrics_volume_path = \"/Volumes/brian_ml_dev/image_processing/training\"\n",
        "\n",
        "# Cluster configuration\n",
        "spark_version = \"17.3.x-scala2.13\"  # MLR 17.3 LTS\n",
        "node_type = \"Standard_NC48ads_A100_v4\"  # Azure NC48ads with 2x A100 GPUs\n",
        "\n",
        "print(f\"Job Configuration:\")\n",
        "print(f\"  Job Name: {job_name}\")\n",
        "print(f\"  Notebook: {notebook_path}\")\n",
        "print(f\"  Init Script: {init_script_path}\")\n",
        "print(f\"  Runtime: {spark_version}\")\n",
        "print(f\"  Node Type: {node_type}\")\n",
        "print(f\"  Max Concurrent Runs: {max_concurrent_runs}\")\n",
        "print(f\"  Metrics Path: {metrics_volume_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "Load the Databricks SDK for job management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks.sdk import WorkspaceClient\n",
        "from databricks.sdk.service import jobs, compute\n",
        "from databricks.sdk.service.compute import Kind\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Workspace Client\n",
        "\n",
        "Create a client to interact with the Databricks workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = WorkspaceClient()\n",
        "print(\"Workspace client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check if Job Already Exists\n",
        "\n",
        "Search for existing jobs with the same name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "existing_jobs = w.jobs.list(name=job_name)\n",
        "existing_job = None\n",
        "\n",
        "for job in existing_jobs:\n",
        "    if job.settings.name == job_name:\n",
        "        existing_job = job\n",
        "        break\n",
        "\n",
        "if existing_job:\n",
        "    print(f\"Job '{job_name}' already exists with ID: {existing_job.job_id}\")\n",
        "else:\n",
        "    print(f\"Job '{job_name}' does not exist\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Job if Not Exists\n",
        "\n",
        "Create the job configuration with notebook task and parallelism settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not existing_job:\n",
        "    # Default parameters matching YOLO multi-GPU notebook\n",
        "    base_parameters = {\n",
        "        \"epochs\": \"2\",\n",
        "        \"batch_size\": \"128\",\n",
        "        \"img_size\": \"640\",\n",
        "        \"initial_lr\": \"0.005\",\n",
        "        \"final_lr\": \"0.1\",\n",
        "        \"device_config\": \"[0,1]\",\n",
        "        \"run_name\": \"multi_gpu_run\"\n",
        "    }\n",
        "    \n",
        "    # Cluster configuration for YOLO training\n",
        "    cluster_config = compute.ClusterSpec(\n",
        "        spark_version=spark_version,\n",
        "        node_type_id=node_type,\n",
        "        num_workers=0,  # Single-node cluster (driver-only)\n",
        "        driver_node_type_id=node_type,\n",
        "        data_security_mode=DataSecurityMode.SINGLE_USER,\n",
        "        kind=Kind.CLASSIC_PREVIEW,\n",
        "        use_ml_runtime=True,\n",
        "        spark_conf={\n",
        "            \"spark.databricks.cluster.profile\": \"singleNode\",\n",
        "            \"spark.master\": \"local[*, 4]\"\n",
        "        },\n",
        "        custom_tags={\n",
        "            \"ResourceClass\": \"SingleNode\"\n",
        "        },\n",
        "        init_scripts=[\n",
        "            compute.InitScriptInfo(\n",
        "                workspace=compute.WorkspaceStorageInfo(\n",
        "                    destination=init_script_path\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    created_job = w.jobs.create(\n",
        "        name=job_name,\n",
        "        tasks=[\n",
        "            jobs.Task(\n",
        "                task_key=\"finetune_task\",\n",
        "                notebook_task=jobs.NotebookTask(\n",
        "                    notebook_path=notebook_path,\n",
        "                    source=jobs.Source.WORKSPACE,\n",
        "                    base_parameters=base_parameters\n",
        "                ),\n",
        "                new_cluster=cluster_config\n",
        "            )\n",
        "        ],\n",
        "        max_concurrent_runs=max_concurrent_runs\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n✅ Job '{job_name}' created successfully\")\n",
        "    print(f\"Job ID: {created_job.job_id}\")\n",
        "    print(f\"Max concurrent runs: {max_concurrent_runs}\")\n",
        "    print(f\"\\nCluster Configuration:\")\n",
        "    print(f\"  Runtime: {spark_version}\")\n",
        "    print(f\"  Node Type: {node_type}\")\n",
        "    print(f\"  Workers: 0 (Single Node)\")\n",
        "    print(f\"  Init Script: {init_script_path.split('/')[-1]}\")\n",
        "    print(f\"\\nDefault parameters:\")\n",
        "    for key, value in base_parameters.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(f\"Job '{job_name}' already exists, skipping creation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Job Details\n",
        "\n",
        "Show the job configuration details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_job = w.jobs.get(existing_job.job_id) if existing_job else w.jobs.get(created_job.job_id)\n",
        "\n",
        "print(f\"\\nJob Configuration:\")\n",
        "print(f\"  Name: {final_job.settings.name}\")\n",
        "print(f\"  Job ID: {final_job.job_id}\")\n",
        "print(f\"  Max Concurrent Runs: {final_job.settings.max_concurrent_runs}\")\n",
        "print(f\"  Notebook Path: {final_job.settings.tasks[0].notebook_task.notebook_path}\")\n",
        "\n",
        "# Display cluster configuration\n",
        "task = final_job.settings.tasks[0]\n",
        "if task.new_cluster:\n",
        "    print(f\"\\nCluster Configuration:\")\n",
        "    print(f\"  Spark Version: {task.new_cluster.spark_version}\")\n",
        "    print(f\"  Node Type: {task.new_cluster.node_type_id}\")\n",
        "    print(f\"  Workers: {task.new_cluster.num_workers}\")\n",
        "    if task.new_cluster.init_scripts:\n",
        "        print(f\"  Init Scripts: {len(task.new_cluster.init_scripts)} configured\")\n",
        "\n",
        "if task.notebook_task.base_parameters:\n",
        "    print(f\"\\nDefault Parameters:\")\n",
        "    for key, value in task.notebook_task.base_parameters.items():\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trigger Job Run\n",
        "\n",
        "Start a new run of the job.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_id = final_job.job_id\n",
        "\n",
        "# Default YOLO parameters for test run\n",
        "test_params = {\n",
        "    \"epochs\": \"2\",\n",
        "    \"batch_size\": \"128\",\n",
        "    \"img_size\": \"640\",\n",
        "    \"initial_lr\": \"0.005\",\n",
        "    \"final_lr\": \"0.1\",\n",
        "    \"device_config\": \"[0,1]\",\n",
        "    \"run_name\": \"test_run\"\n",
        "}\n",
        "\n",
        "run = w.jobs.run_now(\n",
        "    job_id=job_id,\n",
        "    notebook_params=test_params\n",
        ")\n",
        "\n",
        "print(f\"\\nJob run triggered successfully\")\n",
        "print(f\"Run ID: {run.run_id}\")\n",
        "print(f\"Parameters:\")\n",
        "for key, value in test_params.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor Run Status\n",
        "\n",
        "Check the status of the triggered run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "from databricks.sdk.service.jobs import RunLifeCycleState\n",
        "\n",
        "print(f\"Monitoring run {run.run_id}...\")\n",
        "print(f\"Status updates:\")\n",
        "\n",
        "while True:\n",
        "    run_status = w.jobs.get_run(run.run_id)\n",
        "    state = run_status.state.life_cycle_state\n",
        "    \n",
        "    print(f\"  {state}\")\n",
        "    \n",
        "    if state in [RunLifeCycleState.TERMINATED, RunLifeCycleState.SKIPPED, RunLifeCycleState.INTERNAL_ERROR]:\n",
        "        result_state = run_status.state.result_state\n",
        "        print(f\"\\nFinal State: {result_state}\")\n",
        "        \n",
        "        try:\n",
        "            if run_status.tasks and len(run_status.tasks) > 0:\n",
        "                task_run_id = run_status.tasks[0].run_id\n",
        "                print(f\"Retrieving output from task run: {task_run_id}\")\n",
        "                \n",
        "                run_output = w.jobs.get_run_output(task_run_id)\n",
        "                if run_output.notebook_output and run_output.notebook_output.result:\n",
        "                    print(f\"\\nNotebook Output:\")\n",
        "                    metrics = json.loads(run_output.notebook_output.result)\n",
        "                    print(json.dumps(metrics, indent=2))\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCould not retrieve notebook output: {e}\")\n",
        "        \n",
        "        break\n",
        "    \n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimization with Optuna\n",
        "\n",
        "Use Optuna to find the optimal YOLO hyperparameters (batch size, learning rates) by running multiple training experiments in parallel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and Import Optuna\n",
        "\n",
        "Set up Optuna for hyperparameter optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "print(f\"Optuna version: {optuna.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Objective Function\n",
        "\n",
        "Create a function that runs a training job with different hyperparameters and returns the metric to optimize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# job_id = 281960318776979\n",
        "\n",
        "def objective(trial):\n",
        "    import time\n",
        "    import json\n",
        "    \n",
        "    # Sample YOLO hyperparameters\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
        "    initial_lr = trial.suggest_float(\"initial_lr\", 0.001, 0.01, log=True)\n",
        "    final_lr = trial.suggest_float(\"final_lr\", 0.01, 0.5)\n",
        "    \n",
        "    # Fixed parameters for faster experimentation\n",
        "    epochs = 25  # Adjust based on your needs\n",
        "    img_size = 640\n",
        "    device_config = \"[0,1]\"\n",
        "    run_name = f\"trial_{trial.number}\"\n",
        "    \n",
        "    print(f\"\\nTrial {trial.number}:\")\n",
        "    print(f\"  batch_size={batch_size}, initial_lr={initial_lr:.5f}, final_lr={final_lr:.3f}\")\n",
        "    \n",
        "    # Prepare parameters matching the multi-GPU notebook widgets\n",
        "    params = {\n",
        "        \"epochs\": str(epochs),\n",
        "        \"batch_size\": str(batch_size),\n",
        "        \"img_size\": str(img_size),\n",
        "        \"initial_lr\": str(initial_lr),\n",
        "        \"final_lr\": str(final_lr),\n",
        "        \"device_config\": device_config,\n",
        "        \"run_name\": run_name\n",
        "    }\n",
        "    \n",
        "    # Trigger job run\n",
        "    run = w.jobs.run_now(\n",
        "        job_id=job_id,\n",
        "        notebook_params=params\n",
        "    )\n",
        "    \n",
        "    job_run_id = run.run_id\n",
        "    print(f\"  Job Run ID: {job_run_id}\")\n",
        "    print(f\"  Metrics file: {metrics_volume_path}/metrics_{job_run_id}.json\")\n",
        "    \n",
        "    # Wait for job completion\n",
        "    while True:\n",
        "        run_status = w.jobs.get_run(job_run_id)\n",
        "        state = run_status.state.life_cycle_state\n",
        "        \n",
        "        if state in [RunLifeCycleState.TERMINATED, RunLifeCycleState.SKIPPED, RunLifeCycleState.INTERNAL_ERROR]:\n",
        "            result_state = run_status.state.result_state\n",
        "            print(f\"  Run completed: {result_state}\")\n",
        "            \n",
        "            if result_state != jobs.RunResultState.SUCCESS:\n",
        "                print(f\"  Trial failed with state: {result_state}\")\n",
        "                raise optuna.TrialPruned()\n",
        "            \n",
        "            # Read metrics from saved JSON file\n",
        "            try:\n",
        "                time.sleep(5)  # Brief delay to ensure file is written\n",
        "                \n",
        "                metrics_file = f\"{metrics_volume_path}/metrics_{job_run_id}.json\"\n",
        "                print(f\"  Reading metrics from: {metrics_file}\")\n",
        "                \n",
        "                with open(metrics_file, 'r') as f:\n",
        "                    metrics = json.load(f)\n",
        "                \n",
        "                val_map = metrics.get(\"val_mAP50\", 0.0)\n",
        "                print(f\"  Validation mAP@50: {val_map:.4f}\")\n",
        "                \n",
        "                # Return val_mAP50 for maximization\n",
        "                return val_map\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  Error reading metrics file: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "                raise optuna.TrialPruned()\n",
        "            \n",
        "            break\n",
        "        \n",
        "        time.sleep(30)  # Check every 30 seconds (YOLO training takes time)\n",
        "    \n",
        "    raise optuna.TrialPruned()\n",
        "\n",
        "print(\"Objective function defined for YOLO hyperparameter optimization\")\n",
        "print(f\"Metrics will be read from: {metrics_volume_path}/metrics_<run_id>.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Optimization Study\n",
        "\n",
        "Execute Optuna study to find the best n_estimators value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",  # Maximize mAP (we return negative, so minimize negative = maximize)\n",
        "    study_name=\"yolo_hyperparameter_optimization\"\n",
        ")\n",
        "\n",
        "n_trials = 5\n",
        "\n",
        "print(f\"Starting YOLO hyperparameter optimization with {n_trials} trials...\")\n",
        "print(\"Optimizing: Validation mAP@50 (higher is better)\")\n",
        "print(\"Hyperparameters being tuned:\")\n",
        "print(\"  - batch_size: [64, 128, 256]\")\n",
        "print(\"  - initial_lr: [0.001, 0.01] (log scale)\")\n",
        "print(\"  - final_lr: [0.01, 0.5]\")\n",
        "print()\n",
        "\n",
        "# if you use n_jobs then it can run in parallel but we can have issues with databricks access\n",
        "study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "print(f\"\\nOptimization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Optimization Results\n",
        "\n",
        "Show the best parameters and visualize the optimization history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Best Trial:\")\n",
        "print(f\"  Value (Validation mAP@50): {study.best_trial.value:.4f}\")\n",
        "print(f\"  Parameters:\")\n",
        "for key, value in study.best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "print(\"\\nAll Trials:\")\n",
        "trials_df = study.trials_dataframe()\n",
        "# Rename value column to mAP@50 for clarity\n",
        "trials_df['mAP@50'] = trials_df['value']\n",
        "display(trials_df[[\"number\", \"mAP@50\", \"params_batch_size\", \"params_initial_lr\", \"params_final_lr\", \"state\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Optimization History\n",
        "\n",
        "Plot the optimization progress over trials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "trial_numbers = [t.number for t in study.trials if t.value is not None]\n",
        "trial_values = [t.value for t in study.trials if t.value is not None]\n",
        "trial_batch_sizes = [t.params[\"batch_size\"] for t in study.trials if t.value is not None]\n",
        "trial_initial_lrs = [t.params[\"initial_lr\"] for t in study.trials if t.value is not None]\n",
        "\n",
        "# Optimization history\n",
        "ax1.plot(trial_numbers, trial_values, marker='o', linewidth=2)\n",
        "ax1.set_xlabel('Trial Number')\n",
        "ax1.set_ylabel('Validation mAP@50')\n",
        "ax1.set_title('Optimization History')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Batch size vs mAP\n",
        "ax2.scatter(trial_batch_sizes, trial_values, s=100, alpha=0.6)\n",
        "ax2.set_xlabel('Batch Size')\n",
        "ax2.set_ylabel('Validation mAP@50')\n",
        "ax2.set_title('Batch Size vs mAP@50')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# Initial LR vs mAP\n",
        "ax3.scatter(trial_initial_lrs, trial_values, s=100, alpha=0.6)\n",
        "ax3.set_xlabel('Initial Learning Rate')\n",
        "ax3.set_ylabel('Validation mAP@50')\n",
        "ax3.set_title('Initial LR vs mAP@50')\n",
        "ax3.set_xscale('log')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# Mark best trial on all plots\n",
        "best_idx = trial_values.index(max(trial_values))\n",
        "ax2.scatter([trial_batch_sizes[best_idx]], [trial_values[best_idx]], \n",
        "           s=200, color='red', marker='*', label='Best', zorder=5)\n",
        "ax2.legend()\n",
        "ax3.scatter([trial_initial_lrs[best_idx]], [trial_values[best_idx]], \n",
        "           s=200, color='red', marker='*', label='Best', zorder=5)\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nRecommendation:\")\n",
        "print(f\"  Batch Size: {study.best_params['batch_size']}\")\n",
        "print(f\"  Initial LR: {study.best_params['initial_lr']:.5f}\")\n",
        "print(f\"  Final LR: {study.best_params['final_lr']:.3f}\")\n",
        "print(f\"  Best mAP@50: {study.best_trial.value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Working with deployed model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Query Databricks Model Serving Endpoint\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Configuration for Model Serving Endpoint\n",
        "endpoint_name = \"yolo_test\"\n",
        "\n",
        "# Get workspace URL and authentication token\n",
        "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
        "token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
        "\n",
        "# Build the endpoint URL\n",
        "endpoint_url = f\"https://{workspace_url}/serving-endpoints/{endpoint_name}/invocations\"\n",
        "\n",
        "print(f\"Databricks Model Serving Configuration:\")\n",
        "print(f\"  Workspace: {workspace_url}\")\n",
        "print(f\"  Endpoint: {endpoint_name}\")\n",
        "print(f\"  URL: {endpoint_url}\")\n",
        "print(f\"  Token: {'*' * 20} (hidden)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare a sample image for prediction\n",
        "# You can use a sample video frame from your dataset or load any image\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Example: Load a frame from one of the video files\n",
        "video_path = \"/Volumes/brian_ml_dev/image_processing/processed_video/intersection_0.mp4\"\n",
        "\n",
        "# Read a sample frame\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "ret, frame = cap.read()\n",
        "cap.release()\n",
        "\n",
        "if ret:\n",
        "    # Convert BGR to RGB for display\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Display the sample frame\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(frame_rgb)\n",
        "    plt.title(\"Sample Input Frame\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"Frame shape: {frame_rgb.shape}\")\n",
        "    print(f\"Frame dtype: {frame_rgb.dtype}\")\n",
        "else:\n",
        "    print(\"❌ Could not read frame from video\")\n",
        "    # Create a dummy image for testing\n",
        "    frame_rgb = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)\n",
        "    print(\"Using dummy image for testing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions using Model Serving Endpoint with Base64 encoding\n",
        "print(\"Making predictions via Model Serving Endpoint...\")\n",
        "\n",
        "try:\n",
        "    # Convert image to base64 (much smaller payload than raw tensors)\n",
        "    pil_image = Image.fromarray(frame_rgb)\n",
        "    \n",
        "    # Convert to PNG in memory\n",
        "    buffer = BytesIO()\n",
        "    pil_image.save(buffer, format='PNG')\n",
        "    img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "    \n",
        "    payload_size_mb = len(img_base64) / (1024 * 1024)\n",
        "    print(f\"Image encoded to base64\")\n",
        "    print(f\"Original image shape: {frame_rgb.shape}\")\n",
        "    print(f\"Base64 payload size: {payload_size_mb:.2f} MB\")\n",
        "    \n",
        "    # Prepare headers\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "    \n",
        "    # Prepare payload with base64 encoded image\n",
        "    # The model wrapper will handle decoding and preprocessing\n",
        "    payload = {\n",
        "        \"inputs\": {\n",
        "            \"images\": img_base64\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nSending request to: {endpoint_url}\")\n",
        "    print(f\"Payload format: base64 encoded image string\")\n",
        "    print(f\"Input keys: {list(payload['inputs'].keys())}\")\n",
        "    \n",
        "    # Make the prediction request\n",
        "    response = requests.post(\n",
        "        endpoint_url,\n",
        "        headers=headers,\n",
        "        json=payload,\n",
        "        timeout=60\n",
        "    )\n",
        "    \n",
        "    # Check response\n",
        "    if response.status_code == 200:\n",
        "        predictions = response.json()\n",
        "        print(\"✅ Predictions completed successfully\")\n",
        "        print(f\"\\nResponse status: {response.status_code}\")\n",
        "        print(f\"Response type: {type(predictions)}\")\n",
        "        print(f\"\\nPrediction Results:\")\n",
        "        print(json.dumps(predictions, indent=2))\n",
        "    else:\n",
        "        print(f\"❌ Error: HTTP {response.status_code}\")\n",
        "        print(f\"Response: {response.text}\")\n",
        "        predictions = None\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error making predictions: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    predictions = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize detection results on the image\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Configuration: Set confidence threshold (0.0 to 1.0)\n",
        "CONFIDENCE_THRESHOLD = 0.21  # Only show detections with confidence >= 50%\n",
        "\n",
        "try:\n",
        "    if predictions is not None:\n",
        "        # Create a copy for visualization\n",
        "        vis_frame = frame_rgb.copy()\n",
        "        \n",
        "        # Parse the prediction response\n",
        "        detections_found = False\n",
        "        \n",
        "        # The model returns: {\"predictions\": [[[x1,y1,x2,y2,conf,class], ...]]}\n",
        "        if isinstance(predictions, dict) and 'predictions' in predictions:\n",
        "            results = predictions['predictions']  # Get the predictions list\n",
        "            \n",
        "            if isinstance(results, list) and len(results) > 0:\n",
        "                result = results[0]  # First image's detections (batch of 1)\n",
        "                \n",
        "                # Filter out padding (detections with -1 values)\n",
        "                valid_detections = [det for det in result if det[0] != -1.0]\n",
        "                \n",
        "                # Filter by confidence threshold\n",
        "                high_conf_detections = [det for det in valid_detections \n",
        "                                       if float(det[4]) >= CONFIDENCE_THRESHOLD]\n",
        "                \n",
        "                print(f\"\\nTotal detections: {len(valid_detections)}\")\n",
        "                print(f\"High-confidence detections (>={CONFIDENCE_THRESHOLD}): {len(high_conf_detections)}\")\n",
        "                \n",
        "                if len(high_conf_detections) > 0:\n",
        "                    detections_found = True\n",
        "                    \n",
        "                    # Draw each high-confidence detection\n",
        "                    for det in high_conf_detections:\n",
        "                        # Box format: [x1, y1, x2, y2, confidence, class_id]\n",
        "                        x1, y1, x2, y2 = map(int, det[:4])\n",
        "                        conf = float(det[4])\n",
        "                        cls = int(det[5])\n",
        "                        \n",
        "                        # Color based on confidence (green = high, yellow = medium)\n",
        "                        if conf >= 0.75:\n",
        "                            color = (0, 255, 0)      # Green for very high confidence\n",
        "                        elif conf >= 0.6:\n",
        "                            color = (255, 255, 0)    # Yellow for good confidence\n",
        "                        else:\n",
        "                            color = (255, 165, 0)    # Orange for acceptable confidence\n",
        "                        \n",
        "                        # Draw rectangle\n",
        "                        cv2.rectangle(vis_frame, (x1, y1), (x2, y2), color, 2)\n",
        "                        \n",
        "                        # Add label with confidence\n",
        "                        label = f\"Class {cls}: {conf:.2%}\"  # Show as percentage\n",
        "                        cv2.putText(vis_frame, label, (x1, y1 - 10), \n",
        "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "                else:\n",
        "                    print(f\"⚠️ No detections above confidence threshold of {CONFIDENCE_THRESHOLD}\")\n",
        "                    print(f\"Try lowering the threshold. Max confidence: {max([d[4] for d in valid_detections]):.2%}\")\n",
        "        \n",
        "        # Display the result\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.imshow(vis_frame)\n",
        "        plt.title(f\"Detection Results - {endpoint_name} (conf >= {CONFIDENCE_THRESHOLD})\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "        \n",
        "        if detections_found:\n",
        "            print(f\"\\n✅ Visualization complete - {len(high_conf_detections)} high-confidence objects displayed\")\n",
        "        else:\n",
        "            print(\"\\n⚠️ No high-confidence detections found\")\n",
        "            print(\"Try adjusting CONFIDENCE_THRESHOLD or check the prediction response\")\n",
        "    else:\n",
        "        print(\"No predictions to visualize (prediction request failed)\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Error during visualization: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Alternative: Load Model Locally with MLflow\n",
        "\n",
        "Instead of querying the Model Serving endpoint, you can also load the model locally using MLflow. This is useful for development and testing but not recommended for production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Load model locally with MLflow (uncomment to use)\n",
        "# This approach loads the model into memory instead of using the serving endpoint\n",
        "\n",
        "\"\"\"\n",
        "import mlflow\n",
        "\n",
        "# Configuration\n",
        "model_name = \"yolo_test\"\n",
        "model_alias = \"champion\"  # or use a specific version like \"1\"\n",
        "\n",
        "print(f\"Loading model locally: {model_name}\")\n",
        "\n",
        "# Load the model using MLflow\n",
        "model_uri = f\"models:/{model_name}@{model_alias}\"\n",
        "print(f\"Model URI: {model_uri}\")\n",
        "\n",
        "try:\n",
        "    # Load the model\n",
        "    loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
        "    print(\"✅ Model loaded successfully\")\n",
        "    \n",
        "    # Display model metadata\n",
        "    model_info = mlflow.models.get_model_info(model_uri)\n",
        "    print(f\"\\nModel Information:\")\n",
        "    print(f\"  Name: {model_info.name}\")\n",
        "    print(f\"  Version: {model_info.version}\")\n",
        "    print(f\"  Run ID: {model_info.run_id}\")\n",
        "    \n",
        "    # Make predictions\n",
        "    predictions = loaded_model.predict(pil_image)\n",
        "    print(f\"\\nPredictions: {predictions}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading model: {e}\")\n",
        "    print(f\"\\nMake sure the model '{model_name}' is registered and has an alias '{model_alias}'\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"Local MLflow model loading example is available (commented out)\")\n",
        "print(\"The main approach above uses Model Serving endpoint which is recommended for production\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary: Model Serving Endpoint Usage\n",
        "# \n",
        "# The cells above demonstrate:\n",
        "# 1. Configure connection to Databricks Model Serving endpoint (yolo_test)\n",
        "# 2. Load a sample image from video\n",
        "# 3. Send prediction request to the endpoint with base64-encoded image\n",
        "# 4. Parse and visualize the detection results\n",
        "#\n",
        "# For production use, consider:\n",
        "# - Batch processing multiple images\n",
        "# - Error handling and retry logic\n",
        "# - Caching endpoint URL and token\n",
        "# - Monitoring request latency and success rates\n",
        "\n",
        "print(\"✅ Model Serving endpoint example complete\")\n",
        "print(\"Run the cells above to query the 'yolo_test' endpoint with your own images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
