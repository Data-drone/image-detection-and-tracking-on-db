{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# YOLO Hyperparameter Tuning Job\n",
        "\n",
        "This notebook creates a Databricks job to run the YOLO multi-GPU training notebook with hyperparameter optimization using Optuna.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters\n",
        "\n",
        "Define job configuration parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -U databricks-sdk optuna\n",
        "%restart_python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_name = \"finetune_yolo_job\"\n",
        "notebook_path = \"/Workspace/Users/brian.law@databricks.com/.bundle/image-detection-and-tracking-on-db/dev/files/advanced_examples/finetune_ultralytics_yolo_multi_gpu\"\n",
        "init_script_path = \"/Workspace/Users/brian.law@databricks.com/.bundle/image-detection-and-tracking-on-db/dev/files/scripts/init_script_ultralytics.sh\"\n",
        "max_concurrent_runs = 10\n",
        "\n",
        "# Cluster configuration\n",
        "spark_version = \"17.3.x-gpu-ml-scala2.12\"  # MLR 17.3 LTS\n",
        "node_type = \"Standard_NC48ads_A100_v4\"  # Azure NC48ads with 2x A100 GPUs\n",
        "\n",
        "print(f\"Job Configuration:\")\n",
        "print(f\"  Job Name: {job_name}\")\n",
        "print(f\"  Notebook: {notebook_path}\")\n",
        "print(f\"  Init Script: {init_script_path}\")\n",
        "print(f\"  Runtime: {spark_version}\")\n",
        "print(f\"  Node Type: {node_type}\")\n",
        "print(f\"  Max Concurrent Runs: {max_concurrent_runs}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "Load the Databricks SDK for job management.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from databricks.sdk import WorkspaceClient\n",
        "from databricks.sdk.service import jobs, compute\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize Workspace Client\n",
        "\n",
        "Create a client to interact with the Databricks workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = WorkspaceClient()\n",
        "print(\"Workspace client initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check if Job Already Exists\n",
        "\n",
        "Search for existing jobs with the same name.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "existing_jobs = w.jobs.list(name=job_name)\n",
        "existing_job = None\n",
        "\n",
        "for job in existing_jobs:\n",
        "    if job.settings.name == job_name:\n",
        "        existing_job = job\n",
        "        break\n",
        "\n",
        "if existing_job:\n",
        "    print(f\"Job '{job_name}' already exists with ID: {existing_job.job_id}\")\n",
        "else:\n",
        "    print(f\"Job '{job_name}' does not exist\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Job if Not Exists\n",
        "\n",
        "Create the job configuration with notebook task and parallelism settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not existing_job:\n",
        "    # Default parameters matching YOLO multi-GPU notebook\n",
        "    base_parameters = {\n",
        "        \"epochs\": \"2\",\n",
        "        \"batch_size\": \"128\",\n",
        "        \"img_size\": \"640\",\n",
        "        \"initial_lr\": \"0.005\",\n",
        "        \"final_lr\": \"0.1\",\n",
        "        \"device_config\": \"[0,1]\",\n",
        "        \"run_name\": \"multi_gpu_run\"\n",
        "    }\n",
        "    \n",
        "    # Cluster configuration for YOLO training\n",
        "    cluster_config = compute.ClusterSpec(\n",
        "        spark_version=spark_version,\n",
        "        node_type_id=node_type,\n",
        "        num_workers=0,  # Single-node cluster (driver-only)\n",
        "        driver_node_type_id=node_type,\n",
        "        spark_conf={\n",
        "            \"spark.databricks.cluster.profile\": \"singleNode\",\n",
        "            \"spark.master\": \"local[*, 4]\"\n",
        "        },\n",
        "        custom_tags={\n",
        "            \"ResourceClass\": \"SingleNode\"\n",
        "        },\n",
        "        init_scripts=[\n",
        "            compute.InitScriptInfo(\n",
        "                workspace=compute.WorkspaceStorageInfo(\n",
        "                    destination=init_script_path\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    created_job = w.jobs.create(\n",
        "        name=job_name,\n",
        "        tasks=[\n",
        "            jobs.Task(\n",
        "                task_key=\"finetune_task\",\n",
        "                notebook_task=jobs.NotebookTask(\n",
        "                    notebook_path=notebook_path,\n",
        "                    source=jobs.Source.WORKSPACE,\n",
        "                    base_parameters=base_parameters\n",
        "                ),\n",
        "                new_cluster=cluster_config\n",
        "            )\n",
        "        ],\n",
        "        max_concurrent_runs=max_concurrent_runs\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nâœ… Job '{job_name}' created successfully\")\n",
        "    print(f\"Job ID: {created_job.job_id}\")\n",
        "    print(f\"Max concurrent runs: {max_concurrent_runs}\")\n",
        "    print(f\"\\nCluster Configuration:\")\n",
        "    print(f\"  Runtime: {spark_version}\")\n",
        "    print(f\"  Node Type: {node_type}\")\n",
        "    print(f\"  Workers: 0 (Single Node)\")\n",
        "    print(f\"  Init Script: {init_script_path.split('/')[-1]}\")\n",
        "    print(f\"\\nDefault parameters:\")\n",
        "    for key, value in base_parameters.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "else:\n",
        "    print(f\"Job '{job_name}' already exists, skipping creation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Job Details\n",
        "\n",
        "Show the job configuration details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_job = existing_job if existing_job else w.jobs.get(created_job.job_id)\n",
        "\n",
        "print(f\"\\nJob Configuration:\")\n",
        "print(f\"  Name: {final_job.settings.name}\")\n",
        "print(f\"  Job ID: {final_job.job_id}\")\n",
        "print(f\"  Max Concurrent Runs: {final_job.settings.max_concurrent_runs}\")\n",
        "print(f\"  Notebook Path: {final_job.settings.tasks[0].notebook_task.notebook_path}\")\n",
        "\n",
        "# Display cluster configuration\n",
        "task = final_job.settings.tasks[0]\n",
        "if task.new_cluster:\n",
        "    print(f\"\\nCluster Configuration:\")\n",
        "    print(f\"  Spark Version: {task.new_cluster.spark_version}\")\n",
        "    print(f\"  Node Type: {task.new_cluster.node_type_id}\")\n",
        "    print(f\"  Workers: {task.new_cluster.num_workers}\")\n",
        "    if task.new_cluster.init_scripts:\n",
        "        print(f\"  Init Scripts: {len(task.new_cluster.init_scripts)} configured\")\n",
        "\n",
        "if task.notebook_task.base_parameters:\n",
        "    print(f\"\\nDefault Parameters:\")\n",
        "    for key, value in task.notebook_task.base_parameters.items():\n",
        "        print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trigger Job Run\n",
        "\n",
        "Start a new run of the job.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "job_id = final_job.job_id\n",
        "\n",
        "# Default YOLO parameters for test run\n",
        "test_params = {\n",
        "    \"epochs\": \"2\",\n",
        "    \"batch_size\": \"128\",\n",
        "    \"img_size\": \"640\",\n",
        "    \"initial_lr\": \"0.005\",\n",
        "    \"final_lr\": \"0.1\",\n",
        "    \"device_config\": \"[0,1]\",\n",
        "    \"run_name\": \"test_run\"\n",
        "}\n",
        "\n",
        "run = w.jobs.run_now(\n",
        "    job_id=job_id,\n",
        "    notebook_params=test_params\n",
        ")\n",
        "\n",
        "print(f\"\\nJob run triggered successfully\")\n",
        "print(f\"Run ID: {run.run_id}\")\n",
        "print(f\"Parameters:\")\n",
        "for key, value in test_params.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Monitor Run Status\n",
        "\n",
        "Check the status of the triggered run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "from databricks.sdk.service.jobs import RunLifeCycleState\n",
        "\n",
        "print(f\"Monitoring run {run.run_id}...\")\n",
        "print(f\"Status updates:\")\n",
        "\n",
        "while True:\n",
        "    run_status = w.jobs.get_run(run.run_id)\n",
        "    state = run_status.state.life_cycle_state\n",
        "    \n",
        "    print(f\"  {state}\")\n",
        "    \n",
        "    if state in [RunLifeCycleState.TERMINATED, RunLifeCycleState.SKIPPED, RunLifeCycleState.INTERNAL_ERROR]:\n",
        "        result_state = run_status.state.result_state\n",
        "        print(f\"\\nFinal State: {result_state}\")\n",
        "        \n",
        "        try:\n",
        "            if run_status.tasks and len(run_status.tasks) > 0:\n",
        "                task_run_id = run_status.tasks[0].run_id\n",
        "                print(f\"Retrieving output from task run: {task_run_id}\")\n",
        "                \n",
        "                run_output = w.jobs.get_run_output(task_run_id)\n",
        "                if run_output.notebook_output and run_output.notebook_output.result:\n",
        "                    print(f\"\\nNotebook Output:\")\n",
        "                    metrics = json.loads(run_output.notebook_output.result)\n",
        "                    print(json.dumps(metrics, indent=2))\n",
        "        except Exception as e:\n",
        "            print(f\"\\nCould not retrieve notebook output: {e}\")\n",
        "        \n",
        "        break\n",
        "    \n",
        "    time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Optimization with Optuna\n",
        "\n",
        "Use Optuna to find the optimal YOLO hyperparameters (batch size, learning rates) by running multiple training experiments in parallel.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and Import Optuna\n",
        "\n",
        "Set up Optuna for hyperparameter optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "\n",
        "print(f\"Optuna version: {optuna.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Objective Function\n",
        "\n",
        "Create a function that runs a training job with different hyperparameters and returns the metric to optimize.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    # Sample YOLO hyperparameters\n",
        "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
        "    initial_lr = trial.suggest_float(\"initial_lr\", 0.001, 0.01, log=True)\n",
        "    final_lr = trial.suggest_float(\"final_lr\", 0.01, 0.5)\n",
        "    \n",
        "    # Fixed parameters for faster experimentation\n",
        "    epochs = 10  # Adjust based on your needs\n",
        "    img_size = 640\n",
        "    device_config = \"[0,1]\"\n",
        "    run_name = f\"trial_{trial.number}\"\n",
        "    \n",
        "    print(f\"\\nTrial {trial.number}:\")\n",
        "    print(f\"  batch_size={batch_size}, initial_lr={initial_lr:.5f}, final_lr={final_lr:.3f}\")\n",
        "    \n",
        "    params = {\n",
        "        \"epochs\": str(epochs),\n",
        "        \"batch_size\": str(batch_size),\n",
        "        \"img_size\": str(img_size),\n",
        "        \"initial_lr\": str(initial_lr),\n",
        "        \"final_lr\": str(final_lr),\n",
        "        \"device_config\": device_config,\n",
        "        \"run_name\": run_name\n",
        "    }\n",
        "    \n",
        "    run = w.jobs.run_now(\n",
        "        job_id=job_id,\n",
        "        notebook_params=params\n",
        "    )\n",
        "    \n",
        "    print(f\"  Run ID: {run.run_id}\")\n",
        "    \n",
        "    while True:\n",
        "        run_status = w.jobs.get_run(run.run_id)\n",
        "        state = run_status.state.life_cycle_state\n",
        "        \n",
        "        if state in [RunLifeCycleState.TERMINATED, RunLifeCycleState.SKIPPED, RunLifeCycleState.INTERNAL_ERROR]:\n",
        "            result_state = run_status.state.result_state\n",
        "            print(f\"  Run completed: {result_state}\")\n",
        "            \n",
        "            if result_state != jobs.RunResultState.SUCCESS:\n",
        "                raise optuna.TrialPruned()\n",
        "            \n",
        "            try:\n",
        "                # Get metrics from MLflow run\n",
        "                # You may need to adjust this based on what metrics YOLO logs\n",
        "                if run_status.tasks and len(run_status.tasks) > 0:\n",
        "                    task_run_id = run_status.tasks[0].run_id\n",
        "                    run_output = w.jobs.get_run_output(task_run_id)\n",
        "                    \n",
        "                    if run_output.notebook_output and run_output.notebook_output.result:\n",
        "                        metrics = json.loads(run_output.notebook_output.result)\n",
        "                        # Assuming YOLO returns validation mAP or loss\n",
        "                        # Adjust metric name based on what your training returns\n",
        "                        val_map = metrics.get(\"val_mAP50\", 0.0)\n",
        "                        \n",
        "                        print(f\"  Validation mAP@50: {val_map:.4f}\")\n",
        "                        \n",
        "                        # Return negative mAP because Optuna minimizes by default\n",
        "                        return -val_map\n",
        "            except Exception as e:\n",
        "                print(f\"  Error retrieving metrics: {e}\")\n",
        "                raise optuna.TrialPruned()\n",
        "            \n",
        "            break\n",
        "        \n",
        "        time.sleep(30)  # Check every 30 seconds (YOLO training is longer)\n",
        "    \n",
        "    raise optuna.TrialPruned()\n",
        "\n",
        "print(\"Objective function defined for YOLO hyperparameter optimization\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Optimization Study\n",
        "\n",
        "Execute Optuna study to find the best n_estimators value.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",  # Maximize mAP (we return negative, so minimize negative = maximize)\n",
        "    study_name=\"yolo_hyperparameter_optimization\"\n",
        ")\n",
        "\n",
        "n_trials = 5\n",
        "\n",
        "print(f\"Starting YOLO hyperparameter optimization with {n_trials} trials...\")\n",
        "print(\"Optimizing: Validation mAP@50 (higher is better)\")\n",
        "print(\"Hyperparameters being tuned:\")\n",
        "print(\"  - batch_size: [64, 128, 256]\")\n",
        "print(\"  - initial_lr: [0.001, 0.01] (log scale)\")\n",
        "print(\"  - final_lr: [0.01, 0.5]\")\n",
        "print()\n",
        "\n",
        "study.optimize(objective, n_trials=n_trials, n_jobs=2)\n",
        "\n",
        "print(f\"\\nOptimization complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Display Optimization Results\n",
        "\n",
        "Show the best parameters and visualize the optimization history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Best Trial:\")\n",
        "print(f\"  Value (Validation mAP@50): {-study.best_trial.value:.4f}\")  # Negate to show actual mAP\n",
        "print(f\"  Parameters:\")\n",
        "for key, value in study.best_trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "print(\"\\nAll Trials:\")\n",
        "trials_df = study.trials_dataframe()\n",
        "# Convert value back to positive mAP for display\n",
        "trials_df['mAP@50'] = -trials_df['value']\n",
        "display(trials_df[[\"number\", \"mAP@50\", \"params_batch_size\", \"params_initial_lr\", \"params_final_lr\", \"state\"]])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Optimization History\n",
        "\n",
        "Plot the optimization progress over trials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "trial_numbers = [t.number for t in study.trials if t.value is not None]\n",
        "trial_values = [t.value for t in study.trials if t.value is not None]\n",
        "trial_params = [t.params[\"n_estimators\"] for t in study.trials if t.value is not None]\n",
        "\n",
        "ax1.plot(trial_numbers, trial_values, marker='o')\n",
        "ax1.set_xlabel('Trial Number')\n",
        "ax1.set_ylabel('Test RMSE')\n",
        "ax1.set_title('Optimization History')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.scatter(trial_params, trial_values, s=100, alpha=0.6)\n",
        "ax2.set_xlabel('n_estimators')\n",
        "ax2.set_ylabel('Test RMSE')\n",
        "ax2.set_title('Parameter vs Metric')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "best_idx = trial_values.index(min(trial_values))\n",
        "ax2.scatter([trial_params[best_idx]], [trial_values[best_idx]], \n",
        "           s=200, color='red', marker='*', label='Best', zorder=5)\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nRecommendation: Use n_estimators={study.best_params['n_estimators']} for best performance\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
