{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Spark to distribute computer vision\n",
    "\n",
    "In computer vision, a common task is to bulk process videos and run detection algorithms on them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U timm transformers ffmpeg torchcodec\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First lets configure the location of video files and UC Catalog / Schema etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import DetrFeatureExtractor, DetrForObjectDetection, DetrImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "db_catalog = 'brian_ml_dev'\n",
    "db_schema = 'image_processing'\n",
    "processed_videos = 'processed_video'\n",
    "data_table = 'silver_detr_results'\n",
    "\n",
    "video_path = f'/Volumes/{db_catalog}/{db_schema}/{processed_videos}'\n",
    "print(video_path)\n",
    "\n",
    "# quick review videos\n",
    "video_files = os.listdir(video_path)\n",
    "full_path = [os.path.join(video_path, x) for x in video_files ]\n",
    "# video_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To distribute the processing of the video files, we need to create a spark dataframe with all the filepaths for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"src\", StringType(), True)\n",
    "])\n",
    "\n",
    "sourcing_df = spark.createDataFrame([(item,) for item in full_path], schema=schema)\n",
    "display(sourcing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Models for testing functions\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "model.eval()\n",
    "model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Processing - Torchcodec\n",
    "\n",
    "PyTorch is the standard for deep learning currently. Torch has it's own Torchcodec library that can be more efficient than opencv is a torch model is being used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchcodec.decoders import VideoDecoder\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "# With this code, we do the preprocessing via torch modules so we don't need the detr feature extractor\n",
    "\n",
    "# we need to add frame index etc\n",
    "def process_file_w_torchcodec(video_path: str, model, processor, batch_size=8, device='cuda'):\n",
    "    \n",
    "    reader = VideoDecoder(video_path)\n",
    "    \n",
    "    first_frame = reader[0]\n",
    "    image_size = (first_frame.shape[1], first_frame.shape[2])\n",
    "    #reader.set_current_stream(\"video\")\n",
    "\n",
    "    # Preallocate storage\n",
    "    frames = []\n",
    "    frame_indices = []\n",
    "    batch = []\n",
    "    results = []\n",
    "    \n",
    "    for idx, frame in enumerate(reader):\n",
    "        \n",
    "        ### debug line \n",
    "        if idx >=30:\n",
    "            break\n",
    "        #frame_tensor = frame['data']  # Shape: (C, H, W), dtype: uint8\n",
    "        frame_tensor = convert_image_dtype(frame, dtype=torch.float32)  # Normalize [0,1]\n",
    "\n",
    "        image_size = (first_frame.shape[1], first_frame.shape[2])\n",
    "\n",
    "        # Resize if needed (e.g., model requires specific input size)\n",
    "        frame_tensor = Resize((480, 854))(frame_tensor)  # Example: resize to 854x480\n",
    "\n",
    "        # we would need to adjust this\n",
    "        frames.append(frame_tensor)\n",
    "        frame_indices.append(idx)\n",
    "        batch.append(frame_tensor)\n",
    "\n",
    "        if len(batch) == batch_size:\n",
    "            batch_tensor = torch.stack(batch).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(batch_tensor)\n",
    "                \n",
    "                # we need to understand this function a bit more....\n",
    "                processed_outputs = processor.post_process_object_detection(\n",
    "                    outputs,\n",
    "                    threshold=0.5,  # Score threshold\n",
    "                    target_sizes=torch.tensor([image_size] * len(batch))\n",
    "                )\n",
    "\n",
    "            for i, frame_output in enumerate(processed_outputs):\n",
    "                #print(frame_output.keys())\n",
    "\n",
    "                annotations = []\n",
    "\n",
    "                for score, label, box in zip(frame_output['scores'].cpu().numpy(),\n",
    "                                             frame_output['labels'].cpu().numpy(),\n",
    "                                             frame_output['boxes'].cpu().numpy()):\n",
    "                \n",
    "                    annotations.append({\n",
    "                        \"frame_index\": frame_indices[i],\n",
    "                        \"score\": score,\n",
    "                        'label': label,\n",
    "                        'box': box\n",
    "                    })\n",
    "\n",
    "                results.append({\n",
    "                    'video_path': video_path,\n",
    "                    'frame': frames[i].cpu().numpy(),\n",
    "                    'frame_index': frame_indices[i],\n",
    "                    'annotations': annotations\n",
    "\n",
    "                })\n",
    "                \n",
    "            batch.clear()\n",
    "\n",
    "    # Process remaining frames\n",
    "    if batch:\n",
    "        print('entering last batch')\n",
    "        batch_tensor = torch.stack(batch).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_tensor)\n",
    "\n",
    "            processed_final_outputs = processor.post_process_object_detection(\n",
    "                outputs,\n",
    "                threshold=0.5,  # Score threshold\n",
    "                target_sizes=torch.tensor([image_size] * len(batch_tensor))\n",
    "            )\n",
    "\n",
    "            for i, frame_output in enumerate(processed_final_outputs):\n",
    "                annotations = []\n",
    "\n",
    "                for score, label, box in zip(frame_output['scores'].cpu().numpy(),\n",
    "                                             frame_output['labels'].cpu().numpy(),\n",
    "                                             frame_output['boxes'].cpu().numpy()):\n",
    "                \n",
    "                    annotations.append({\n",
    "                        'frame_index': frame_indices[i],\n",
    "                        'score': float(score.item()),\n",
    "                        'label': label,\n",
    "                        'box': [float(v) for v in box] \n",
    "                    })\n",
    "\n",
    "                results.append({\n",
    "                    'video_path': video_path,\n",
    "                    'frame': frames[i].cpu().numpy(),\n",
    "                    'frame_index': frame_indices[i],\n",
    "                    'annotations': annotations\n",
    "\n",
    "                })\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_check = os.path.join(video_path, os.listdir(video_path)[0])\n",
    "print(file_to_check)\n",
    "\n",
    "results = process_file_w_torchcodec(file_to_check, model, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing on Spark Cluster\n",
    "\n",
    "Now that we have tested our functions, we can distribute it across a full spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model Function - to start the process\n",
    "\n",
    "model = None\n",
    "feature_extractor = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Load model per worker process (lazy initialization).\"\"\"\n",
    "    global model, feature_extractor, processor\n",
    "    if model is None or feature_extractor is None or processor is None:\n",
    "        feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "        \n",
    "        model.eval()\n",
    "        model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run a python function across a pyspark cluster we need to wrap it into a pandas udf for best performance and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# udf transformers\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, FloatType, IntegerType, StringType, BinaryType\n",
    "import io\n",
    "\n",
    "def python_batch_process(file_iter: pd.Series) -> pd.DataFrame:\n",
    "\n",
    "    # run the model load\n",
    "    load_model()\n",
    "\n",
    "    for file_batch in file_iter:\n",
    "        batch_results = []\n",
    "\n",
    "        for video_path in file_batch[\"src\"]:\n",
    "            video_results = process_file_w_torchcodec(video_path, model, processor)\n",
    "\n",
    "            frames = []\n",
    "            annotations = []\n",
    "            frame_indexes = []\n",
    "\n",
    "            for dict_obj in video_results:\n",
    "                frame = dict_obj[\"frame\"]\n",
    "                frame_index = dict_obj[\"frame_index\"]\n",
    "                frame_annotations = dict_obj[\"annotations\"]\n",
    "\n",
    "                try:\n",
    "                # Step 1: Transpose dimensions to (height, width, channels)\n",
    "                    frame_transposed = np.transpose(frame, (1, 2, 0))\n",
    "\n",
    "                    # Step 2: Convert to uint8 (if values are in 0–1, scale to 0–255 first)\n",
    "                    if frame_transposed.dtype == np.float32:\n",
    "                        frame_transposed = (frame_transposed * 255).astype(np.uint8)\n",
    "                    else:\n",
    "                        frame_transposed = frame_transposed.astype(np.uint8)\n",
    "\n",
    "                    try:\n",
    "                        img = Image.fromarray(frame_transposed)\n",
    "                        byte_stream = io.BytesIO()\n",
    "                        img.save(byte_stream, format=\"PNG\")\n",
    "                        frame_bytes = byte_stream.getvalue()\n",
    "\n",
    "                        dict_obj[\"frame\"] = frame_bytes\n",
    "                        dict_obj[\"frame_index\"] = float(dict_obj[\"frame_index\"])\n",
    "\n",
    "                    except TypeError:\n",
    "                        print('failed')\n",
    "                        failed_block = dict_obj\n",
    "                        #print(dict_obj)\n",
    "                        break\n",
    "        \n",
    "                except ValueError:\n",
    "                    print('failed reshaping')\n",
    "                    failed_block = dict_obj\n",
    "                    break\n",
    "\n",
    "        yield pd.DataFrame(video_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sourcing_df.mapInPandas(python_batch_process, schema=StructType([\n",
    "        StructField(\"video_path\", StringType(), True),\n",
    "        StructField(\"frame\", BinaryType(), True),  # Encoded images\n",
    "        StructField(\"frame_index\", IntegerType(), True),\n",
    "        StructField(\"annotations\", ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"frame_index\", IntegerType(), True),\n",
    "                StructField(\"score\", FloatType(), True),\n",
    "                StructField(\"label\", IntegerType(), True),\n",
    "                StructField(\"box\", ArrayType(FloatType()), True)\n",
    "            ])\n",
    "        ), True)\n",
    "    ]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Issue with the frame indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.write.mode('overwrite').saveAsTable(f\"`{db_catalog}`.`{db_schema}`.silver_detr_results_w_frame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to COCO Output for Finetuning\n",
    "\n",
    "We can convert the output to coco format for finetuning as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(f\"SELECT * FROM {db_catalog}.{db_schema}.silver_detr_results_w_frame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "# Load config from pretrained RT-DETR model\n",
    "config = AutoConfig.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "# Get category mappings\n",
    "id2label = config.id2label\n",
    "label2id = config.label2id\n",
    "\n",
    "# Convert to COCO categories format\n",
    "base_coco_categories = [\n",
    "    {\"id\": int(k), \"name\": v, \"supercategory\": \"none\"}\n",
    "    for k, v in id2label.items()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import uuid\n",
    "from PIL import Image\n",
    "\n",
    "storage_location = 'coco_dataset'\n",
    "\n",
    "output_root = f'/Volumes/{db_catalog}/{db_schema}/{storage_location}'\n",
    "images_dir = os.path.join(output_root, \"images\")\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "coco = {\n",
    "    \"images\": [],\n",
    "    \"annotations\": [],\n",
    "    \"categories\": []\n",
    "}\n",
    "\n",
    "used_category_ids = set()\n",
    "annotation_id = 1\n",
    "\n",
    "df_collected = result.select(\"video_path\", \"frame\", \"annotations\").collect()\n",
    "\n",
    "for idx, row in enumerate(df_collected):\n",
    "    frame_bytes = row[\"frame\"]\n",
    "    annotations = row[\"annotations\"]\n",
    "\n",
    "    image_id = idx\n",
    "    filename = f\"image_{idx}.jpg\"\n",
    "    file_path = os.path.join(images_dir, filename)\n",
    "\n",
    "    # Save image\n",
    "    img = Image.open(io.BytesIO(frame_bytes))\n",
    "    img.save(file_path)\n",
    "    width, height = img.size\n",
    "\n",
    "    # Image entry\n",
    "    coco[\"images\"].append({\n",
    "        \"id\": idx,\n",
    "        \"file_name\": f\"images/{filename}\",\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "\n",
    "    # Annotations\n",
    "    if annotations:\n",
    "        for ann in annotations:\n",
    "            category_id = int(ann[\"label\"])\n",
    "            used_category_ids.add(category_id)\n",
    "\n",
    "            x, y, x2, y2 = ann[\"box\"]\n",
    "            w, h = x2 - x, y2 - y\n",
    "\n",
    "            coco[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": idx,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"area\": w * h,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "# Add categories\n",
    "coco[\"categories\"] = [\n",
    "    cat for cat in base_coco_categories if cat[\"id\"] in used_category_ids\n",
    "]\n",
    "\n",
    "# Save to JSON\n",
    "with open(os.path.join(output_root, \"annotations.json\"), \"w\") as f:\n",
    "    json.dump(coco, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
