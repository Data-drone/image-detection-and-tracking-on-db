{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Videos with Spark\n",
    "\n",
    "This noetbook will focus on converting video formats and scaling that process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ffmpeg\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "\n",
    "## Setup and Config\n",
    "\n",
    "We will setup db catalogs schemas and locations for files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "catalog = 'brian_ml_dev'\n",
    "schema = 'image_processing'\n",
    "raw_videos = 'raw_data'\n",
    "\n",
    "destination_folder = 'processed_video'\n",
    "\n",
    "raw_path = f'/Volumes/{catalog}/{schema}/{raw_videos}'\n",
    "processed_path = f'/Volumes/{catalog}/{schema}/{destination_folder}'\n",
    "\n",
    "# check contents\n",
    "file_list = os.listdir(raw_path)\n",
    "print(f'Files Available: {file_list}:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing the processing - Setup Paths\n",
    "\n",
    "In order to distribute the processing, we need to:\n",
    "- convert the metadata into a spark dataframe with source / destination / optional flags\n",
    "\n",
    "Optional Flags depends on how we structure our udf. If we want to be able to set custom options depending on file then we would have an optional flags column with a dict or other structure so that we can feed it into our udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = [x.split('.')[0] for x in file_list]\n",
    "full_path = [os.path.join(raw_path, x+'.mp4') for x in filenames ]\n",
    "dest_paths = [os.path.join(processed_path, x+'.mp4') for x in filenames]\n",
    "\n",
    "print(len(dest_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"src\", StringType(), True),\n",
    "    StructField(\"dest\", StringType(), True),\n",
    "])\n",
    "\n",
    "sourcing_df = spark.createDataFrame(list(x for x in zip(full_path, dest_paths)), schema=schema)\n",
    "display(sourcing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, IntegerType, BinaryType, BooleanType\n",
    "import pandas as pd\n",
    "\n",
    "def check_file_exists(file_path:str) -> bool:\n",
    "    \"\"\"\n",
    "    returns true or files based on if a file is existing or not\n",
    "    \"\"\"\n",
    "    if os.path.exists(file_path):\n",
    "        if os.path.isfile(file_path):\n",
    "            #print(f\"The file {file_path} exists.\")\n",
    "            return True\n",
    "        else:\n",
    "            #print(f\"{file_path} exists but it is not a file.\")\n",
    "            return False\n",
    "    else:\n",
    "        #print(f\"The file {file_path} does not exist.\")\n",
    "        return False\n",
    "\n",
    "# udf wrapper for the function above\n",
    "@pandas_udf(BooleanType())\n",
    "def check_file_esists_udf(input: pd.Series) -> pd.Series:\n",
    "    return input.apply(check_file_exists)\n",
    "\n",
    "\n",
    "file_check = sourcing_df.withColumn('file_exists', check_file_esists_udf(col('dest')))\n",
    "display(file_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing out processing function\n",
    "\n",
    "It is always best to test functions first before we roll it out to scale on a spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "source = f\"{raw_path}/{file_list[0]}\"\n",
    "dest_path = '/local_disk0/tt2/20241029-024247-3.mp4'\n",
    "command = ['ffmpeg', '-i', source, '-filter:v', 'crop=1920:1200:0:0', dest_path]\n",
    "#'ffmpeg -i /Volumes/prj-orica-video-analytics/data_processing/test_processing/20241029-024247.svo -filter:v \"crop=1920:1200:0:0\" /local_disk0/tt2/20241029-024247-3.mp4'\n",
    "\n",
    "text = subprocess.run(command, check=True, capture_output=True, text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing File Processing\n",
    "\n",
    "Now that we know the right ffmpeg command we can distribute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "import subprocess\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def process_video(src: str, dest: str) -> dict:\n",
    "\n",
    "    # temp file cache\n",
    "    db_expandable_storage = \"/local_disk0\"\n",
    "\n",
    "    with tempfile.TemporaryDirectory(dir=db_expandable_storage) as tmp_dir:\n",
    "        tmp_output_path = os.path.join(tmp_dir, \"output.mp4\")  # Adjust filename if needed\n",
    "\n",
    "        # Construct the ffmpeg command\n",
    "        command = ['ffmpeg', '-i', src, '-filter:v', 'crop=1920:1200:0:0', tmp_output_path]\n",
    "\n",
    "        try:\n",
    "            # Run ffmpeg command\n",
    "            command_result = subprocess.run(command, check=True, capture_output=True, text=True)\n",
    "\n",
    "            # Copy the processed file to cloud storage\n",
    "            shutil.copy(tmp_output_path, dest)\n",
    "\n",
    "            return {\n",
    "                'command_args': ' '.join(command_result.args),\n",
    "                'return_code': command_result.returncode,\n",
    "                'stdout': command_result.stdout,\n",
    "                'stderr': command_result.stderr\n",
    "            }\n",
    "\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            return {\n",
    "                'command_args': ' '.join(e.cmd),\n",
    "                'return_code': e.returncode,\n",
    "                'stdout': '',\n",
    "                'stderr': e.stderr\n",
    "            }\n",
    "        except Exception as ex:\n",
    "            return {\n",
    "                'command_args': ' '.join(command),\n",
    "                'return_code': -1,\n",
    "                'stdout': '',\n",
    "                'stderr': str(ex)\n",
    "            }\n",
    "\n",
    "# to convert a function into a pyfunc we need to define the output schema in PySpark terms\n",
    "schema = StructType([\n",
    "    StructField(\"command_args\", StringType(), True),\n",
    "    StructField(\"return_code\", IntegerType(), True),\n",
    "    StructField(\"stdout\", StringType(), True),\n",
    "    StructField(\"stderr\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Register and wrap the python function into a udf wrapper\n",
    "process_video_udf = udf(process_video, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = file_check \\\n",
    "    .filter(col('file_exists') == False) \\\n",
    "    .withColumn(\"video_processing\", process_video_udf(file_check.src, file_check.dest))\n",
    "\n",
    "display(df.select(\"video_processing.*\").show(truncate=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
